{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Manual with Creating token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input dengan lebih dari dua segmen\n",
    "segments = [\n",
    "    \"What is photosynthesis?\",  # Segmen 1\n",
    "    \"Photosynthesis is the process by which plants convert sunlight into energy.\",  # Segmen 2\n",
    "    \"It occurs in the chloroplasts of plant cells.\"  # Segmen 3\n",
    "]\n",
    "\n",
    "# Encode masing-masing segmen\n",
    "encoded_segments = [tokenizer.encode(seg, add_special_tokens=False) for seg in segments]\n",
    "\n",
    "# Gabungkan segmen dengan [SEP] di antaranya\n",
    "input_ids = [tokenizer.cls_token_id]  # [CLS]\n",
    "token_type_ids = []  # Untuk menyimpan ID tipe token\n",
    "current_segment_id = 0\n",
    "\n",
    "for segment in encoded_segments:\n",
    "    input_ids.extend(segment + [tokenizer.sep_token_id])  # Tambahkan segmen dan [SEP]\n",
    "    token_type_ids.extend([current_segment_id] * (len(segment) + 1))  # Token Type IDs\n",
    "    current_segment_id += 1  # Pindah ke segmen berikutnya\n",
    "\n",
    "# Padding untuk mencapai panjang maksimum\n",
    "max_length = 50\n",
    "attention_mask = [1] * len(input_ids)  # Mask untuk token yang relevan\n",
    "\n",
    "# Tambahkan padding jika diperlukan\n",
    "while len(input_ids) < max_length:\n",
    "    input_ids.append(0)  # Token PAD\n",
    "    attention_mask.append(0)\n",
    "    token_type_ids.append(0)  # Token Type ID untuk padding\n",
    "\n",
    "# Pastikan panjangnya sesuai\n",
    "input_ids = input_ids[:max_length]\n",
    "attention_mask = attention_mask[:max_length]\n",
    "token_type_ids = token_type_ids[:max_length]\n",
    "\n",
    "# Konversi ke tensor PyTorch\n",
    "input_ids = torch.tensor([input_ids])\n",
    "attention_mask = torch.tensor([attention_mask])\n",
    "token_type_ids = torch.tensor([token_type_ids])\n",
    "\n",
    "# Output\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Attention Mask:\", attention_mask)\n",
    "print(\"Token Type IDs:\", token_type_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding custom embedding to handle more than 2 segment in BERT model varian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AlbertModel\n",
    "\n",
    "class CustomAlbertModel(nn.Module):\n",
    "    def __init__(self, model_name='indobenchmark/indobert-lite-base-p2', num_token_types=3):\n",
    "        super().__init__()\n",
    "        # Load ALBERT model\n",
    "        self.albert = AlbertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Replace token_type_embeddings to support more token types\n",
    "        self.albert.embeddings.token_type_embeddings = nn.Embedding(\n",
    "            num_embeddings=num_token_types, \n",
    "            embedding_dim=self.albert.config.hidden_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # Pass through modified ALBERT model\n",
    "        return self.albert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "# Create model instance\n",
    "model = CustomAlbertModel(num_token_types=3)\n",
    "\n",
    "# Test model\n",
    "input_ids = torch.randint(0, 30000, (4, 512))  # Example input\n",
    "attention_mask = torch.ones(4, 512)  # Example mask\n",
    "token_type_ids = torch.randint(0, 3, (4, 512))  # Example token types (0, 1, 2)\n",
    "\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "print(output.last_hidden_state.shape)  # Should be (batch_size, sequence_length, hidden_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
