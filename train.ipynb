{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "# \n",
    "from dataset import EssayDataset\n",
    "from longDataset import LongEssayDataset\n",
    "from bert_regression import BertRegressionModel\n",
    "from hierarchicalBert import HierarchicalBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>max_length1</th>\n",
       "      <th>normalized_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>sumber tenaga, pemanis alami, menjaga sistem i...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>65</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>sebagai sumber energi, pemanis alami, menjaga ...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>66</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>1. Sebagai energi. 2. Sebagai memperlancaar pe...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>76</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>untuk membuat kenyang, agar tidak lapar, agar ...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>67</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>Karbohidrat mempunyai peran penting untuk pros...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>105</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question  \\\n",
       "0  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "1  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "2  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "3  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "4  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "\n",
       "                                    reference_answer  \\\n",
       "0  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "1  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "2  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "3  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "4  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "\n",
       "                                              answer  score         dataset  \\\n",
       "0  sumber tenaga, pemanis alami, menjaga sistem i...   27.0  analisis_essay   \n",
       "1  sebagai sumber energi, pemanis alami, menjaga ...   21.0  analisis_essay   \n",
       "2  1. Sebagai energi. 2. Sebagai memperlancaar pe...   42.0  analisis_essay   \n",
       "3  untuk membuat kenyang, agar tidak lapar, agar ...   18.0  analisis_essay   \n",
       "4  Karbohidrat mempunyai peran penting untuk pros...   82.0  analisis_essay   \n",
       "\n",
       "   max_length1  normalized_score  \n",
       "0           65              0.27  \n",
       "1           66              0.21  \n",
       "2           76              0.42  \n",
       "3           67              0.18  \n",
       "4          105              0.82  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset/aes_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2558 entries, 19376 to 21933\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   question          2558 non-null   object \n",
      " 1   reference_answer  2558 non-null   object \n",
      " 2   answer            2558 non-null   object \n",
      " 3   score             2558 non-null   float64\n",
      " 4   dataset           2558 non-null   object \n",
      " 5   max_length1       2558 non-null   int64  \n",
      " 6   normalized_score  2558 non-null   float64\n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 159.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_sag = df[df['dataset'] == 'sag']\n",
    "df_sag.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12 entries, 19376 to 19387\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   question          12 non-null     object \n",
      " 1   reference_answer  12 non-null     object \n",
      " 2   answer            12 non-null     object \n",
      " 3   score             12 non-null     float64\n",
      " 4   dataset           12 non-null     object \n",
      " 5   max_length1       12 non-null     int64  \n",
      " 6   normalized_score  12 non-null     float64\n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 768.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_test = df_sag[:12]\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['max_length1'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# load indobert tokenizer\n",
    "tokenizer1 = BertTokenizer.from_pretrained(\"indobenchmark/indobert-lite-base-p2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply dataset with overlapping 128 and max length 512\n",
    "test_dataset = LongEssayDataset(test, tokenizer1, 512, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset to dataloader for creating batch size 4\n",
    "dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=lambda x: list(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HierarchicalBert(\n",
       "  (bert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertSdpaAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (regression_layer): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model, optimizer, and loss function (MSE)\n",
    "model = HierarchicalBert(\"indobenchmark/indobert-lite-base-p2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.MSELoss()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:404: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attention_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked chunks  tensor([[ 0.2031, -0.1543,  0.7005,  ..., -0.3017,  0.8910,  0.2358],\n",
      "        [ 0.0139, -0.0067,  0.1115,  ...,  0.0378, -0.0385,  0.1059],\n",
      "        [ 0.0197, -0.0284,  0.1244,  ...,  0.0484, -0.0558,  0.0994]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "stacked chunks transponse tensor([[ 0.2031,  0.0139,  0.0197],\n",
      "        [-0.1543, -0.0067, -0.0284],\n",
      "        [ 0.7005,  0.1115,  0.1244],\n",
      "        ...,\n",
      "        [-0.3017,  0.0378,  0.0484],\n",
      "        [ 0.8910, -0.0385, -0.0558],\n",
      "        [ 0.2358,  0.1059,  0.0994]], device='cuda:0',\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "attention weight  tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7090, 0.2910],\n",
      "        [0.0000, 0.6807, 0.3193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "pooled output  tensor([[ 0.2031, -0.1543,  0.7005,  ..., -0.3017,  0.8910,  0.2358],\n",
      "        [ 0.0156, -0.0130,  0.1152,  ...,  0.0409, -0.0435,  0.1040],\n",
      "        [ 0.0157, -0.0136,  0.1156,  ...,  0.0412, -0.0440,  0.1038]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n",
      "mean pooled output  tensor([ 7.8146e-02, -6.0318e-02,  3.1043e-01,  2.0614e-01,  9.8390e-02,\n",
      "        -9.7306e-02, -3.0873e-01,  2.0269e-02, -1.8870e-01, -5.7917e-01,\n",
      "        -3.1419e-01,  3.3236e-01, -7.2451e-02,  9.9413e-02,  7.9218e-02,\n",
      "        -1.6124e-01,  1.6969e-01, -4.6517e-01,  3.0715e-02, -3.6729e-02,\n",
      "        -5.8854e-02,  8.1362e-02,  2.0856e-01,  2.8919e-01, -1.8772e-01,\n",
      "         8.5651e-01, -2.8552e-01,  4.1255e-01, -2.8184e-01,  4.8773e-01,\n",
      "         2.7122e-02,  5.3084e-01,  2.3209e-01,  1.6223e-01, -3.9543e-01,\n",
      "        -1.2280e-01, -2.9028e-01,  1.8430e-01, -1.4347e-01, -3.3611e-02,\n",
      "         3.4829e-02,  2.1432e-01, -5.3596e-01, -5.5001e-01, -3.1812e-01,\n",
      "         2.2061e-01, -9.8451e-02,  2.0980e-01, -4.0740e-01, -1.6113e-01,\n",
      "        -3.5227e-02, -3.5512e-01, -5.5033e-01, -1.3331e-01, -3.6165e-01,\n",
      "         5.5440e-01,  6.4820e-02,  2.5243e-01, -6.4589e-02, -5.6916e-01,\n",
      "         3.6359e-01,  1.7447e-01,  3.7171e-01,  4.7376e-01, -2.4382e-02,\n",
      "        -1.8041e-01,  3.2906e-01,  3.7443e-01,  1.3192e-01, -1.4067e-01,\n",
      "         1.5946e-01,  1.7597e-01,  6.1902e-01,  8.2311e-02,  2.8980e-01,\n",
      "         1.9061e-01, -6.7635e-03,  3.5553e-02, -4.9631e-02, -1.1174e-01,\n",
      "         4.3899e-01, -4.4834e-01, -3.3506e-01,  3.2229e-01,  5.0753e-01,\n",
      "        -1.9545e-01,  4.7778e-03,  1.2003e-01,  5.7123e-02, -2.8961e-01,\n",
      "        -3.0594e-01,  3.2787e-01,  5.8720e-02, -4.2686e-01, -1.8363e-01,\n",
      "         4.0562e-01, -8.3793e-02,  3.0748e-01,  1.9089e-01,  2.4718e-01,\n",
      "         3.0181e-01,  9.0533e-02,  3.4824e-01,  2.0946e-01, -3.8570e-03,\n",
      "        -7.1624e-03,  3.6372e-02,  2.9548e-01,  1.7619e-01, -5.3287e-02,\n",
      "         4.2400e-01,  4.0532e-01, -1.7787e-01, -1.0238e-01, -1.5938e-01,\n",
      "         8.3294e-02,  5.9347e-01,  1.3295e-01,  8.0751e-02,  1.9521e-01,\n",
      "         3.5721e-01, -2.9681e-01,  5.9485e-02,  1.1881e-01,  1.6697e-01,\n",
      "         8.8412e-02,  8.3276e-01,  3.4789e-01,  1.3982e-01,  3.4697e-01,\n",
      "         4.3176e-01,  2.6651e-01, -3.7837e-01,  2.9627e-01, -2.3818e-01,\n",
      "         4.7361e-01, -2.0051e-01,  9.6125e-02, -9.3822e-02,  4.6192e-01,\n",
      "         4.5147e-01, -2.3561e-01, -5.2333e-01,  4.6865e-01,  3.5605e-02,\n",
      "        -2.3694e-02, -7.5585e-02, -4.3780e-01,  4.7667e-02,  5.3653e-02,\n",
      "        -8.5397e-01,  7.6277e-02, -2.4638e-01, -1.2944e-01, -5.5093e-02,\n",
      "         4.5735e-01,  2.5108e-01,  4.6916e-02, -1.0396e-01,  1.1715e-01,\n",
      "         1.9932e-01,  5.1770e-02,  1.5817e-01, -7.8188e-02, -1.4080e-01,\n",
      "        -1.4072e-01,  1.5745e-01,  4.1575e-01, -2.6337e-01, -4.0795e-02,\n",
      "        -1.3679e-01,  2.3436e-01, -1.5897e-02, -2.0677e-01,  3.0417e-01,\n",
      "        -1.2427e-02,  1.5987e-01,  1.0233e-01,  4.6888e-02, -3.2958e-02,\n",
      "        -1.3509e-01,  1.3655e-01, -1.7968e-01,  1.5329e-01, -4.4548e-01,\n",
      "        -7.1416e-02,  1.6347e-01, -2.1971e-01, -8.4460e-02, -1.4441e-01,\n",
      "        -1.4564e-01, -1.4348e-01,  1.5373e-01,  8.6713e-02, -4.8072e-03,\n",
      "         1.7736e-01,  4.2282e-01, -2.2190e-01,  1.9898e-01, -5.5437e-02,\n",
      "         3.6055e-02, -3.1510e-01, -1.4683e-01,  3.0732e-01,  1.8736e+00,\n",
      "         2.2401e-01,  1.4115e-01,  1.7172e-01, -9.1786e-01, -4.5627e-02,\n",
      "        -3.7159e-01,  3.9153e-01, -1.8500e-01, -4.8531e-01,  1.3644e-01,\n",
      "        -2.0964e-01,  1.8200e-01,  2.1797e-01,  5.7111e-02,  3.6848e-01,\n",
      "         4.1874e-02,  2.7142e-02, -1.5513e-02, -3.7434e-01,  3.5805e-01,\n",
      "        -2.4402e-01,  1.1445e-01,  3.2330e-02,  5.6895e-02,  9.8606e-02,\n",
      "         3.7187e-02,  4.5707e-02, -3.7820e-01, -1.1637e-01, -3.0985e-01,\n",
      "        -1.1639e-01,  1.2654e-01,  1.4984e-01, -6.9325e-02, -1.4895e-01,\n",
      "         1.1516e-01,  4.7335e-02,  5.6500e-01,  2.4012e-01,  3.6660e-02,\n",
      "         6.0100e-02,  5.0729e-02,  1.9470e-01,  5.8987e-01, -3.9842e-01,\n",
      "        -3.3090e-01,  7.7523e-02, -3.4357e-01, -3.7136e-01, -2.3924e-01,\n",
      "        -2.7589e-01,  4.6705e-01, -5.5021e-01, -2.1517e-01, -6.4356e-01,\n",
      "         6.0363e-01, -6.7294e-02, -3.2238e-01,  5.8119e-02, -1.4341e-01,\n",
      "         4.7902e-01, -1.4154e-01,  1.0576e-01,  4.3830e-02,  2.3759e-01,\n",
      "         2.4250e-01,  6.6532e-02,  9.4746e-02,  1.8155e-01,  3.0493e-01,\n",
      "         2.8080e-01,  2.6420e-01, -1.0768e-01, -3.3116e-01, -7.7354e-01,\n",
      "        -9.3217e-01,  2.1594e-01, -4.7207e-01,  3.6313e-01,  5.6987e-02,\n",
      "        -3.3866e-01,  2.9916e-03, -3.0916e-01, -1.5400e-01,  5.8826e-01,\n",
      "        -4.7618e-01,  3.0141e-01, -4.1231e-02, -1.8940e-01, -3.0341e-01,\n",
      "         1.0610e-01,  2.2681e-01, -4.9885e-02,  1.3232e-01,  4.7498e-02,\n",
      "        -1.9744e-01, -3.8381e-02,  2.7593e-01, -8.8879e-02,  3.1285e-01,\n",
      "        -3.1167e-01, -3.8760e-01,  1.5832e-02,  1.5536e-01,  5.4415e-01,\n",
      "         5.0589e-01,  6.4136e-03, -1.0682e+00,  1.2750e-01,  2.1386e-01,\n",
      "         4.3389e-01, -2.2635e-01,  5.9042e-01,  6.7292e-01, -1.5788e-01,\n",
      "         4.9639e-01, -3.4662e-01,  1.4184e-01,  6.1846e-01, -1.5722e-01,\n",
      "        -1.1235e-01,  8.8394e-02,  1.2832e-01, -2.1936e-01,  3.0873e-01,\n",
      "        -6.4461e-02,  9.6042e-02,  2.4121e-01,  2.5774e-01,  3.5037e-01,\n",
      "        -1.5470e-01, -3.5795e-01,  2.3002e-01, -7.2817e-01,  4.5026e-02,\n",
      "         2.3695e-01,  4.9579e-01, -5.0988e+00, -5.0057e-01,  2.4698e-01,\n",
      "        -2.0940e-02,  3.9113e-01,  2.2660e-01, -4.9518e-01,  4.6897e-02,\n",
      "         2.4767e-02, -1.0168e-01,  2.8074e-01,  1.5058e-01, -1.0219e-01,\n",
      "         2.9128e-01, -3.7729e-02,  1.3330e-01,  2.5984e-01, -4.8007e-01,\n",
      "         4.4048e-01, -1.0587e-01, -3.7513e-01, -4.4224e-01,  2.2716e-01,\n",
      "         6.9056e-02, -4.8863e-01, -1.1211e-01, -9.1705e-02,  4.5507e-01,\n",
      "         1.7141e-01, -1.1288e-01,  2.0740e-01,  4.8466e-02,  1.7686e-01,\n",
      "         2.4696e-01,  1.3743e-01, -3.0892e-01, -1.7533e-01,  1.6339e-02,\n",
      "        -3.9572e-01, -4.0779e-02,  1.2222e-01,  3.5252e-01, -1.9222e-01,\n",
      "        -7.1975e-02, -2.4766e-01,  1.7449e-02, -6.4271e-02,  1.4033e-02,\n",
      "         7.2253e-01, -3.1437e-01, -5.9480e-03,  1.3879e-01, -4.2685e-01,\n",
      "         2.5220e-01,  4.1025e-01, -3.2320e-02,  2.1852e-02, -5.8185e-01,\n",
      "        -3.7458e-02, -1.2579e-01, -7.5820e-01,  2.8282e-01, -7.8337e-02,\n",
      "         1.1555e-01,  4.1445e-02, -2.4932e-01, -1.1625e-01,  1.7508e-01,\n",
      "         7.9664e-02, -1.6659e-01, -2.9469e-01,  8.2944e-02, -1.2458e-01,\n",
      "        -1.0094e-01,  1.3538e-01,  2.0608e-01,  5.8871e-02,  2.8258e-01,\n",
      "         1.0610e-01,  5.6584e-02, -1.5258e-01,  2.2066e-01, -4.4398e-01,\n",
      "        -1.1391e-03,  4.7312e-01,  2.5114e-01, -7.3548e-02,  9.3765e-02,\n",
      "         4.5740e-01,  7.6803e-02, -4.8653e-01, -1.6002e-02, -9.9471e-02,\n",
      "        -1.2037e-01, -4.1904e-01,  2.6876e-01,  2.6403e-01,  2.8843e-01,\n",
      "         2.2419e-02,  2.1438e-01, -6.5256e-02,  2.5974e-01, -5.8085e-02,\n",
      "         2.7156e-01, -1.1430e-01, -6.4963e-02,  7.4953e-02, -9.7740e-02,\n",
      "        -1.1513e-01, -2.3453e-02, -3.7909e-03,  3.7113e-01, -5.4123e-01,\n",
      "        -7.9937e-02, -5.2411e-01, -1.9152e-02, -4.3208e-01, -2.6442e-01,\n",
      "        -8.8301e-02, -5.6318e-02,  4.4772e-01,  4.6548e-01,  5.0674e-02,\n",
      "         2.7025e-01, -1.1841e-01,  3.7269e-01,  1.7981e-01,  4.3862e-01,\n",
      "        -3.6627e-01,  2.5501e-01,  4.5086e-01,  2.6912e-01,  6.1581e-01,\n",
      "        -2.1313e-01,  1.1047e-01,  1.1481e-01, -1.1940e-01, -7.7078e-02,\n",
      "         3.9177e-01, -3.3964e-01,  2.3040e-01,  4.4551e-01, -2.9174e-01,\n",
      "         9.8023e-02,  1.1178e-01,  1.0248e-02,  9.0312e-02, -3.4798e-01,\n",
      "         4.1907e-03, -1.0152e-02, -4.7308e-02,  4.2154e-01,  2.2561e-01,\n",
      "         6.1283e-01,  1.3052e-01,  3.1772e-01, -1.1062e-01, -2.7656e-01,\n",
      "         2.1794e-01, -1.8542e-01,  1.4136e-01, -3.1233e+00,  1.0061e-01,\n",
      "         9.9209e-02,  4.4519e-01,  2.7079e-01, -1.9123e-01, -2.5650e-01,\n",
      "         3.1922e-01,  5.6294e-02, -6.0123e-02, -3.0706e-01, -1.9481e-01,\n",
      "         3.2145e-01,  7.1905e-02, -3.3136e-02,  1.0693e-01,  2.1569e-01,\n",
      "        -1.3089e-01,  5.6666e-01, -1.6423e-01, -2.1593e-01, -2.8969e-01,\n",
      "         1.8026e-02,  1.1334e-01, -5.2533e-01,  1.3128e-01, -1.3111e-01,\n",
      "        -3.9116e-01, -2.3266e-02, -5.1439e-01, -3.0351e-02,  3.7252e-01,\n",
      "        -8.0170e-01, -1.0483e-01, -1.6641e-01, -1.0326e-02, -8.5461e-02,\n",
      "         3.0032e-01, -2.1438e-01,  7.4145e-01, -8.5464e-03, -1.5846e-01,\n",
      "        -1.5278e-01,  4.0084e-01,  3.3394e-01,  5.3354e-01, -7.2541e-02,\n",
      "         4.3017e-01,  3.4443e-02,  8.5492e-02,  2.5887e-01, -5.3025e-02,\n",
      "         1.1291e-01, -1.4489e-01,  1.0716e-01,  5.8745e-01, -7.9594e-02,\n",
      "        -1.1214e-01, -8.4212e-02, -3.5708e-01,  3.4918e-01,  3.1957e-01,\n",
      "        -2.1735e-01, -4.5267e-02,  4.9157e-01,  2.8716e-01, -1.6458e-01,\n",
      "        -5.4556e-01,  6.4720e-01,  1.1902e-02, -1.0982e-01, -3.5209e-01,\n",
      "         3.5323e-02, -2.7407e-01,  1.7038e-01,  6.2492e-02, -1.8250e-02,\n",
      "        -2.3314e-02, -1.5129e-01,  2.9887e-01,  2.0861e-01,  3.7480e-01,\n",
      "         1.0972e-02,  1.1119e-01, -1.9546e-01,  2.1365e-01,  1.6369e-02,\n",
      "         2.2775e-01, -7.1726e-01, -9.5600e-02, -5.8678e-02, -1.3399e-01,\n",
      "        -1.9634e-01,  5.6370e-02,  6.5276e-02,  2.0448e-01, -2.8546e-01,\n",
      "        -9.4483e-02, -4.3473e-01,  1.4707e-01,  5.9988e-02, -1.7745e-01,\n",
      "        -1.6886e-01, -1.9673e-01, -4.9466e-02,  4.3486e-02, -3.5369e-01,\n",
      "         1.0473e-01, -2.4569e-01, -2.3516e-01, -2.9008e-01, -3.6467e-01,\n",
      "        -1.3118e-01,  2.6949e-01, -2.0619e-01, -1.2761e-01,  5.1131e-01,\n",
      "        -6.3948e-01, -2.6202e-01,  1.1251e-01, -1.3674e-01,  7.9721e-02,\n",
      "         2.6726e-02, -9.8965e-02, -3.0168e-01,  5.0213e-02, -2.5636e-01,\n",
      "         3.2450e-02, -6.3432e-02, -1.1946e-01,  2.4921e-01,  2.3134e-01,\n",
      "         3.4583e-01, -2.7669e-01,  6.9639e-02,  1.3302e-01,  3.5227e-01,\n",
      "         2.6519e-01, -5.9865e-02, -7.6385e-02,  8.2256e-03,  6.4970e-02,\n",
      "        -2.9309e-01,  9.4008e-03, -2.3536e-01,  1.2978e-01, -1.1065e-01,\n",
      "         2.8892e-01, -5.0778e-02,  3.0025e-02, -1.1067e-01, -2.2437e-01,\n",
      "         7.3443e-01, -1.6922e-02, -1.1624e-01,  1.7110e-02, -1.9549e-01,\n",
      "         2.3453e-01, -1.4620e-01,  2.8697e-01, -7.4792e-01,  1.0074e-01,\n",
      "        -3.8626e-01,  2.7014e-01, -8.1732e-02,  8.7729e-02,  2.9293e-02,\n",
      "         1.0619e-01, -2.1522e-01, -2.0274e-01,  4.2136e-02,  3.0418e-01,\n",
      "         5.7776e-01, -7.4613e-02, -2.4902e-01,  1.5021e-01, -5.5317e-01,\n",
      "         1.1396e-02, -2.2827e-01,  1.8829e-01,  3.5263e-01,  2.5922e-01,\n",
      "         2.0944e-02, -3.2677e-02,  3.7299e-01,  7.1236e-01,  6.9094e-02,\n",
      "        -2.0534e-01, -3.6115e-02,  3.5017e-01,  1.7603e-01, -2.2077e-01,\n",
      "        -2.7844e-01,  2.1057e-02, -2.5258e-01, -1.4897e-01,  6.4862e-02,\n",
      "        -7.3333e-01, -3.4913e-01,  3.8559e-01, -9.0090e-02,  2.6297e-02,\n",
      "         3.1205e-01, -3.1134e-01,  2.5567e-01,  2.2076e-02,  2.7369e-02,\n",
      "        -5.4941e-01,  3.3142e-02,  4.0799e-02,  1.8408e-01, -3.6284e-01,\n",
      "         2.0660e-01,  2.3737e-01, -2.0602e-01, -1.6903e-01, -4.0872e-01,\n",
      "         1.4170e-02, -4.3935e-01,  2.3868e-01,  2.9911e-01, -3.3385e-01,\n",
      "        -7.6711e-02,  2.0666e-01,  3.2690e-01, -1.8595e-01, -1.1670e-01,\n",
      "         3.1682e-01, -1.2124e-01,  2.0937e-01, -2.2280e-01,  1.2716e-01,\n",
      "         3.0847e-01,  2.5165e-01,  4.1635e-01,  3.1586e-01, -1.4549e-01,\n",
      "         3.6092e-01,  5.5034e-01,  3.6003e-01, -1.2396e-01, -2.1939e-01,\n",
      "         3.2609e-01,  1.0753e-01, -5.5073e-01, -4.1002e-01,  1.1191e-01,\n",
      "         1.1632e-01, -5.6396e-02, -4.0902e-01, -2.5323e-02, -1.0578e-01,\n",
      "        -4.8783e-01, -9.0352e-03, -5.7062e-01, -8.8725e-02,  4.3146e-02,\n",
      "        -4.8236e-01,  3.4290e-01, -3.5527e-01, -6.6360e-01,  6.1229e-02,\n",
      "        -7.3219e-02,  2.6783e-01,  1.4784e-01], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# training process\n",
    "for epoch in range(1):\n",
    "    for batch, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # batch = batch.to(device)\n",
    "        targets = torch.stack(targets).to(device)\n",
    "        # forward pass\n",
    "        predictions = model(batch).squeeze(1)\n",
    "        print(\"Predictions:\", predictions)\n",
    "        print(\"Targets:\", targets)\n",
    "        # compute loss\n",
    "        loss = criterion(predictions, targets)\n",
    "        print(\"loss : \", loss.item())\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
