{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "# \n",
    "from dataset import EssayDataset\n",
    "from longDataset import LongEssayDataset\n",
    "from bert_regression import BertRegressionModel\n",
    "from hierarchicalBert import HierarchicalBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv(\"dataset/analisis_essay.csv\")\n",
    "# df1['dataset'] = 'analisis_essay'\n",
    "# df2 = pd.read_csv(\"dataset/asap.csv\")\n",
    "# df2['dataset'] = 'asap'\n",
    "# df3 = pd.read_csv(\"dataset/cunlp.csv\")\n",
    "# df3['dataset'] = 'cunlp'\n",
    "# df4 = pd.read_csv(\"dataset/sag.csv\")\n",
    "# df4['dataset'] = 'sag'\n",
    "# df5 = pd.read_csv(\"dataset/sci.csv\")\n",
    "# df5['dataset'] = 'sci'\n",
    "# df6 = pd.read_csv(\"dataset/stita.csv\")\n",
    "# df6['dataset'] = 'stita'\n",
    "\n",
    "# # add null value to all dataframe that doesn't have question column\n",
    "# for df in [df2, df3, df6]:\n",
    "#     df['question'] = pd.NA\n",
    "\n",
    "# df = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)\n",
    "# df.shape\n",
    "\n",
    "# df.to_csv(\"dataset/aes_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>max_length1</th>\n",
       "      <th>normalized_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>sumber tenaga, pemanis alami, menjaga sistem i...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>65</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>sebagai sumber energi, pemanis alami, menjaga ...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>66</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>1. Sebagai energi. 2. Sebagai memperlancaar pe...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>76</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>untuk membuat kenyang, agar tidak lapar, agar ...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>67</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>Karbohidrat mempunyai peran penting untuk pros...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>105</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question  \\\n",
       "0  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "1  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "2  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "3  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "4  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "\n",
       "                                    reference_answer  \\\n",
       "0  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "1  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "2  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "3  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "4  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "\n",
       "                                              answer  score         dataset  \\\n",
       "0  sumber tenaga, pemanis alami, menjaga sistem i...   27.0  analisis_essay   \n",
       "1  sebagai sumber energi, pemanis alami, menjaga ...   21.0  analisis_essay   \n",
       "2  1. Sebagai energi. 2. Sebagai memperlancaar pe...   42.0  analisis_essay   \n",
       "3  untuk membuat kenyang, agar tidak lapar, agar ...   18.0  analisis_essay   \n",
       "4  Karbohidrat mempunyai peran penting untuk pros...   82.0  analisis_essay   \n",
       "\n",
       "   max_length1  normalized_score  \n",
       "0           65              0.27  \n",
       "1           66              0.21  \n",
       "2           76              0.42  \n",
       "3           67              0.18  \n",
       "4          105              0.82  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset/aes_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22406 entries, 0 to 22405\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   question          4859 non-null   object \n",
      " 1   reference_answer  22406 non-null  object \n",
      " 2   answer            22406 non-null  object \n",
      " 3   score             22406 non-null  float64\n",
      " 4   dataset           22406 non-null  object \n",
      " 5   max_length1       22406 non-null  int64  \n",
      " 6   normalized_score  22406 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "asap              17043\n",
       "sag                2558\n",
       "analisis_essay     2162\n",
       "stita               333\n",
       "cunlp               171\n",
       "sci                 139\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert NaN to None\n",
    "df['question'] = df['question'].replace({np.nan: None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Max Length of the Sub Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# dataset = EssayDataset(df, tokenizer, 512)\n",
    "\n",
    "# df['max_length'] = df.index.map(dataset.get_max_length)\n",
    "# len(df[(df['max_length'] > 510)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1 = BertTokenizer.from_pretrained(\"indobenchmark/indobert-lite-base-p2\")\n",
    "dataset = EssayDataset(df, tokenizer1, 512)\n",
    "\n",
    "df['max_length1'] = df.index.map(dataset.get_max_length)\n",
    "len(df[(df['dataset'] != 'analisis_essay')&(df['max_length1'] > 510)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22406.000000\n",
       "mean       161.063688\n",
       "std        108.950303\n",
       "min         30.000000\n",
       "25%        114.000000\n",
       "50%        150.000000\n",
       "75%        186.000000\n",
       "max       1832.000000\n",
       "Name: max_length1, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['max_length1'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_score(row, min_max_dict):\n",
    "    min_score, max_score = min_max_dict[row['dataset']]\n",
    "    return (row['score'] - min_score) / (max_score - min_score)\n",
    "\n",
    "# Menghitung min dan max per kategori sebagai tuple\n",
    "min_max_dict = df.groupby('dataset')['score'].agg(['min', 'max']).apply(tuple, axis=1).to_dict()\n",
    "\n",
    "# Menambahkan kolom normalized_score\n",
    "df['normalized_score'] = df.apply(lambda x: normalize_score(x, min_max_dict), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22406.000000\n",
       "mean         0.384133\n",
       "std          0.339251\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.333333\n",
       "75%          0.666667\n",
       "max          1.000000\n",
       "Name: normalized_score, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['normalized_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean dataframe\n",
    "df.to_csv(\"dataset/aes_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2558 entries, 19376 to 21933\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   question          2558 non-null   object \n",
      " 1   reference_answer  2558 non-null   object \n",
      " 2   answer            2558 non-null   object \n",
      " 3   score             2558 non-null   float64\n",
      " 4   dataset           2558 non-null   object \n",
      " 5   max_length1       2558 non-null   int64  \n",
      " 6   normalized_score  2558 non-null   float64\n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 159.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_sag = df[df['dataset'] == 'sag']\n",
    "df_sag.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12 entries, 19376 to 19387\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   question          12 non-null     object \n",
      " 1   reference_answer  12 non-null     object \n",
      " 2   answer            12 non-null     object \n",
      " 3   score             12 non-null     float64\n",
      " 4   dataset           12 non-null     object \n",
      " 5   max_length1       12 non-null     int64  \n",
      " 6   normalized_score  12 non-null     float64\n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 768.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_test = df_sag[:12]\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['max_length1'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = LongEssayDataset(test, tokenizer1, 512, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(test_dataset, batch_size=4, collate_fn=lambda x: list(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HierarchicalBert(\n",
       "  (bert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertSdpaAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (regression_layer): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HierarchicalBert(\"indobenchmark/indobert-lite-base-p2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.MSELoss()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len tensor :  1166\n",
      "len tensor :  1018\n",
      "Predictions: tensor([-0.1692, -0.1689], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.8000], device='cuda:0')\n",
      "loss :  0.9390792846679688\n",
      "len tensor :  1065\n",
      "len tensor :  1070\n",
      "Predictions: tensor([0.1305, 0.1319], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.7548768520355225\n",
      "len tensor :  1020\n",
      "len tensor :  1025\n",
      "Predictions: tensor([0.4216, 0.4258], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 1.0000], device='cuda:0')\n",
      "loss :  0.23644286394119263\n",
      "len tensor :  1056\n",
      "len tensor :  1123\n",
      "Predictions: tensor([0.6767, 0.6784], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.9100], device='cuda:0')\n",
      "loss :  0.03441764786839485\n",
      "len tensor :  1055\n",
      "len tensor :  1045\n",
      "Predictions: tensor([0.8852, 0.8847], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.8000], device='cuda:0')\n",
      "loss :  0.007216768339276314\n",
      "len tensor :  1131\n",
      "len tensor :  1047\n",
      "Predictions: tensor([1.0261, 1.0225], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.9100, 0.9100], device='cuda:0')\n",
      "loss :  0.01307363249361515\n",
      "len tensor :  1057\n",
      "len tensor :  1167\n",
      "Predictions: tensor([1.1004, 1.1027], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.9100], device='cuda:0')\n",
      "loss :  0.02361566387116909\n",
      "len tensor :  1210\n",
      "len tensor :  1014\n",
      "Predictions: tensor([1.1301, 1.0296], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.5900], device='cuda:0')\n",
      "loss :  0.15111839771270752\n",
      "len tensor :  1252\n",
      "len tensor :  1119\n",
      "Predictions: tensor([0.7149, 0.7157], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.9100], device='cuda:0')\n",
      "loss :  0.018986981362104416\n",
      "len tensor :  1115\n",
      "len tensor :  1027\n",
      "Predictions: tensor([0.7073, 0.9968], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.7000], device='cuda:0')\n",
      "loss :  0.04407641291618347\n",
      "len tensor :  1000\n",
      "len tensor :  1096\n",
      "Predictions: tensor([0.9590, 0.6780], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.8000], device='cuda:0')\n",
      "loss :  0.04098251834511757\n",
      "len tensor :  1089\n",
      "len tensor :  1125\n",
      "Predictions: tensor([0.6428, 0.6406], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.8000], device='cuda:0')\n",
      "loss :  0.014348560012876987\n",
      "len tensor :  1829\n",
      "len tensor :  1121\n",
      "Predictions: tensor([0.3922, 0.6649], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.6000], device='cuda:0')\n",
      "loss :  0.18681472539901733\n",
      "len tensor :  1721\n",
      "len tensor :  1766\n",
      "Predictions: tensor([0.5018, 0.5049], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.24665291607379913\n",
      "len tensor :  1762\n",
      "len tensor :  1458\n",
      "Predictions: tensor([0.5300, 0.5307], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.22057586908340454\n",
      "len tensor :  1669\n",
      "len tensor :  1518\n",
      "Predictions: tensor([0.5727, 0.5709], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.7900], device='cuda:0')\n",
      "loss :  0.11530966311693192\n",
      "len tensor :  1582\n",
      "len tensor :  1023\n",
      "Predictions: tensor([0.6169, 1.1242], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.2100], device='cuda:0')\n",
      "loss :  0.49131491780281067\n",
      "len tensor :  1312\n",
      "len tensor :  1245\n",
      "Predictions: tensor([0.8242, 0.8241], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.6000], device='cuda:0')\n",
      "loss :  0.040566205978393555\n",
      "len tensor :  1346\n",
      "len tensor :  1305\n",
      "Predictions: tensor([0.8327, 0.8327], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.7900], device='cuda:0')\n",
      "loss :  0.014916582964360714\n",
      "len tensor :  1324\n",
      "len tensor :  1272\n",
      "Predictions: tensor([0.8436, 0.8433], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.024507611989974976\n",
      "len tensor :  1797\n",
      "len tensor :  1483\n",
      "Predictions: tensor([0.6673, 0.6661], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.1110713928937912\n",
      "len tensor :  1406\n",
      "len tensor :  1722\n",
      "Predictions: tensor([0.9082, 0.7025], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.4000, 1.0000], device='cuda:0')\n",
      "loss :  0.1733851581811905\n",
      "len tensor :  1176\n",
      "len tensor :  1473\n",
      "Predictions: tensor([0.9336, 0.7214], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7900, 1.0000], device='cuda:0')\n",
      "loss :  0.04911821335554123\n",
      "len tensor :  1460\n",
      "len tensor :  1184\n",
      "Predictions: tensor([0.7440, 0.9599], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.7900], device='cuda:0')\n",
      "loss :  0.047213196754455566\n",
      "len tensor :  1514\n",
      "len tensor :  1367\n",
      "Predictions: tensor([0.7638, 0.9852], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.028005508705973625\n",
      "len tensor :  1285\n",
      "len tensor :  1376\n",
      "Predictions: tensor([1.0129, 1.0125], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7900, 1.0000], device='cuda:0')\n",
      "loss :  0.02492215298116207\n",
      "len tensor :  1346\n",
      "len tensor :  1056\n",
      "Predictions: tensor([1.0308, 1.0299], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.4000, 0.2100], device='cuda:0')\n",
      "loss :  0.5350939035415649\n",
      "len tensor :  1345\n",
      "len tensor :  1454\n",
      "Predictions: tensor([1.0061, 0.7819], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.023808453232049942\n",
      "len tensor :  1044\n",
      "len tensor :  1190\n",
      "Predictions: tensor([1.3532, 0.9878], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.4000, 0.4000], device='cuda:0')\n",
      "loss :  0.6270339488983154\n",
      "len tensor :  1656\n",
      "len tensor :  1158\n",
      "Predictions: tensor([0.7204, 0.9278], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.7000], device='cuda:0')\n",
      "loss :  0.06503056734800339\n",
      "len tensor :  1463\n",
      "len tensor :  1359\n",
      "Predictions: tensor([0.6744, 0.8728], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.06108412519097328\n",
      "len tensor :  1215\n",
      "len tensor :  1592\n",
      "Predictions: tensor([0.8354, 0.6459], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7900, 1.0000], device='cuda:0')\n",
      "loss :  0.06370599567890167\n",
      "len tensor :  1322\n",
      "len tensor :  1422\n",
      "Predictions: tensor([0.8130, 0.8132], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.0349312424659729\n",
      "len tensor :  1403\n",
      "len tensor :  1337\n",
      "Predictions: tensor([0.8032, 0.8033], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.7900], device='cuda:0')\n",
      "loss :  0.019457178190350533\n",
      "len tensor :  1143\n",
      "len tensor :  1143\n",
      "Predictions: tensor([0.7987, 0.7983], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.4900, 0.6000], device='cuda:0')\n",
      "loss :  0.06729979068040848\n",
      "len tensor :  1224\n",
      "len tensor :  1421\n",
      "Predictions: tensor([0.7803, 0.7806], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.7900], device='cuda:0')\n",
      "loss :  0.02417096123099327\n",
      "len tensor :  1228\n",
      "len tensor :  1280\n",
      "Predictions: tensor([0.7700, 0.7704], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.6000, 0.7900], device='cuda:0')\n",
      "loss :  0.014643258415162563\n",
      "len tensor :  1718\n",
      "len tensor :  1379\n",
      "Predictions: tensor([0.5853, 0.7398], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.11985041201114655\n",
      "len tensor :  1210\n",
      "len tensor :  1121\n",
      "Predictions: tensor([0.7428, 1.0268], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7900, 1.0000], device='cuda:0')\n",
      "loss :  0.0014757071621716022\n",
      "len tensor :  1218\n",
      "len tensor :  1229\n",
      "Predictions: tensor([0.7443, 0.7442], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.0654180496931076\n",
      "len tensor :  1054\n",
      "len tensor :  1201\n",
      "Predictions: tensor([1.0485, 1.0499], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.2100, 1.0000], device='cuda:0')\n",
      "loss :  0.3527793288230896\n",
      "len tensor :  1275\n",
      "len tensor :  1078\n",
      "Predictions: tensor([0.7462, 1.0120], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.032284658402204514\n",
      "len tensor :  1166\n",
      "len tensor :  1018\n",
      "Predictions: tensor([1.0476, 1.0451], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.8000], device='cuda:0')\n",
      "loss :  0.06069004163146019\n",
      "len tensor :  1065\n",
      "len tensor :  1070\n",
      "Predictions: tensor([1.0201, 1.0206], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.00041444989619776607\n",
      "len tensor :  1020\n",
      "len tensor :  1025\n",
      "Predictions: tensor([0.9908, 0.9936], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 1.0000], device='cuda:0')\n",
      "loss :  0.018215302377939224\n",
      "len tensor :  1056\n",
      "len tensor :  1123\n",
      "Predictions: tensor([0.9590, 0.9617], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.9100], device='cuda:0')\n",
      "loss :  0.013980643823742867\n",
      "len tensor :  1055\n",
      "len tensor :  1045\n",
      "Predictions: tensor([0.9222, 0.9220], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.8000], device='cuda:0')\n",
      "loss :  0.014914458617568016\n",
      "len tensor :  1131\n",
      "len tensor :  1047\n",
      "Predictions: tensor([0.8815, 0.8776], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.9100, 0.9100], device='cuda:0')\n",
      "loss :  0.000932941387873143\n",
      "len tensor :  1057\n",
      "len tensor :  1167\n",
      "Predictions: tensor([0.8405, 0.8440], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.9100], device='cuda:0')\n",
      "loss :  0.014892352744936943\n",
      "len tensor :  1210\n",
      "len tensor :  1014\n",
      "Predictions: tensor([0.8204, 0.8150], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.5900], device='cuda:0')\n",
      "loss :  0.02552027814090252\n",
      "len tensor :  1252\n",
      "len tensor :  1119\n",
      "Predictions: tensor([0.5625, 0.5619], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.9100], device='cuda:0')\n",
      "loss :  0.07003432512283325\n",
      "len tensor :  1115\n",
      "len tensor :  1027\n",
      "Predictions: tensor([0.5571, 0.7820], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.7000], device='cuda:0')\n",
      "loss :  0.01357016060501337\n",
      "len tensor :  1000\n",
      "len tensor :  1096\n",
      "Predictions: tensor([0.7758, 0.5521], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.8000], device='cuda:0')\n",
      "loss :  0.03361332416534424\n",
      "len tensor :  1089\n",
      "len tensor :  1125\n",
      "Predictions: tensor([0.5556, 0.5542], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.8000], device='cuda:0')\n",
      "loss :  0.04063302278518677\n",
      "len tensor :  1829\n",
      "len tensor :  1121\n",
      "Predictions: tensor([0.3540, 0.5832], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.6000], device='cuda:0')\n",
      "loss :  0.20878718793392181\n",
      "len tensor :  1721\n",
      "len tensor :  1766\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(targets)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictions)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTargets:\u001b[39m\u001b[38;5;124m\"\u001b[39m, targets)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[127], line 26\u001b[0m, in \u001b[0;36mHierarchicalBert.forward\u001b[1;34m(self, batch_chunks)\u001b[0m\n\u001b[0;32m     23\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# masuk ke arsitektur bert -> outputnya [max_seq x [768 token]]\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03mambil token CLS nya saja karena pada token ini berisi \"rangkuman\" informasi dari keseluruhan token (token cls ini ada di index 0)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m(alt) 511 token lainnya bisa saja digunakan, namun harus masuk ke mean pooling/attention pooling dulu agar ukuran token berubah dari 512x768 ke 1x768\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m chunk_outputs\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:804\u001b[0m, in \u001b[0;36mAlbertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    800\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m extended_attention_mask) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[0;32m    802\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 804\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    813\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    815\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler_activation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output[:, \u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:535\u001b[0m, in \u001b[0;36mAlbertTransformer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Index of the hidden group\u001b[39;00m\n\u001b[0;32m    533\u001b[0m group_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(i \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_groups))\n\u001b[1;32m--> 535\u001b[0m layer_group_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malbert_layer_groups\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayers_per_group\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayers_per_group\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_group_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:487\u001b[0m, in \u001b[0;36mAlbertLayerGroup.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m    484\u001b[0m layer_attentions \u001b[38;5;241m=\u001b[39m ()\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_index, albert_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malbert_layers):\n\u001b[1;32m--> 487\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[43malbert_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:450\u001b[0m, in \u001b[0;36mAlbertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    444\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     output_hidden_states: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 450\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m     ffn_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_chunk,\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward,\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim,\n\u001b[0;32m    456\u001b[0m         attention_output[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    457\u001b[0m     )\n\u001b[0;32m    458\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_layer_layer_norm(ffn_output \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:393\u001b[0m, in \u001b[0;36mAlbertSdpaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    391\u001b[0m batch_size, seq_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    392\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(hidden_states))\n\u001b[1;32m--> 393\u001b[0m key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    394\u001b[0m value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# Reference: https://github.com/pytorch/pytorch/issues/112577\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    for batch, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # batch = batch.to(device)\n",
    "        targets = torch.stack(targets).to(device)\n",
    "        # forward pass\n",
    "        predictions = model(batch).squeeze(1)\n",
    "\n",
    "        print(\"Predictions:\", predictions)\n",
    "        print(\"Targets:\", targets)\n",
    "        # compute loss\n",
    "        loss = criterion(predictions, targets)\n",
    "        print(\"loss : \", loss.item())\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
