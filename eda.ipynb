{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "# \n",
    "from dataset import EssayDataset\n",
    "from longDataset import LongEssayDataset\n",
    "from bert_regression import BertRegressionModel\n",
    "from hierarchicalBert import HierarchicalBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.read_csv(\"dataset/analisis_essay.csv\")\n",
    "# df1['dataset'] = 'analisis_essay'\n",
    "# df2 = pd.read_csv(\"dataset/asap.csv\")\n",
    "# df2['dataset'] = 'asap'\n",
    "# df3 = pd.read_csv(\"dataset/cunlp.csv\")\n",
    "# df3['dataset'] = 'cunlp'\n",
    "# df4 = pd.read_csv(\"dataset/sag.csv\")\n",
    "# df4['dataset'] = 'sag'\n",
    "# df5 = pd.read_csv(\"dataset/sci.csv\")\n",
    "# df5['dataset'] = 'sci'\n",
    "# df6 = pd.read_csv(\"dataset/stita.csv\")\n",
    "# df6['dataset'] = 'stita'\n",
    "\n",
    "# # add null value to all dataframe that doesn't have question column\n",
    "# for df in [df2, df3, df6]:\n",
    "#     df['question'] = pd.NA\n",
    "\n",
    "# df = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)\n",
    "# df.shape\n",
    "\n",
    "# df.to_csv(\"dataset/aes_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>max_length1</th>\n",
       "      <th>normalized_score</th>\n",
       "      <th>normalized_score2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>sumber tenaga, pemanis alami, menjaga sistem i...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>65</td>\n",
       "      <td>0.27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>sebagai sumber energi, pemanis alami, menjaga ...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>66</td>\n",
       "      <td>0.21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>1. Sebagai energi. 2. Sebagai memperlancaar pe...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>76</td>\n",
       "      <td>0.42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>untuk membuat kenyang, agar tidak lapar, agar ...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>67</td>\n",
       "      <td>0.18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>Karbohidrat mempunyai peran penting untuk pros...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>105</td>\n",
       "      <td>0.82</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question  \\\n",
       "0  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "1  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "2  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "3  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "4  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "\n",
       "                                    reference_answer  \\\n",
       "0  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "1  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "2  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "3  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "4  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "\n",
       "                                              answer  score         dataset  \\\n",
       "0  sumber tenaga, pemanis alami, menjaga sistem i...   27.0  analisis_essay   \n",
       "1  sebagai sumber energi, pemanis alami, menjaga ...   21.0  analisis_essay   \n",
       "2  1. Sebagai energi. 2. Sebagai memperlancaar pe...   42.0  analisis_essay   \n",
       "3  untuk membuat kenyang, agar tidak lapar, agar ...   18.0  analisis_essay   \n",
       "4  Karbohidrat mempunyai peran penting untuk pros...   82.0  analisis_essay   \n",
       "\n",
       "   max_length1  normalized_score  normalized_score2  \n",
       "0           65              0.27                 27  \n",
       "1           66              0.21                 21  \n",
       "2           76              0.42                 42  \n",
       "3           67              0.18                 18  \n",
       "4          105              0.82                 82  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset/aes_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22406 entries, 0 to 22405\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   question           4859 non-null   object \n",
      " 1   reference_answer   22406 non-null  object \n",
      " 2   answer             22406 non-null  object \n",
      " 3   score              22406 non-null  float64\n",
      " 4   dataset            22406 non-null  object \n",
      " 5   max_length1        22406 non-null  int64  \n",
      " 6   normalized_score   22406 non-null  float64\n",
      " 7   normalized_score2  22406 non-null  int64  \n",
      "dtypes: float64(2), int64(2), object(4)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "asap              17043\n",
       "sag                2558\n",
       "analisis_essay     2162\n",
       "stita               333\n",
       "cunlp               171\n",
       "sci                 139\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert NaN to None\n",
    "df['question'] = df['question'].replace({np.nan: None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Max Length of the Sub Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# dataset = EssayDataset(df, tokenizer, 512)\n",
    "\n",
    "# df['max_length'] = df.index.map(dataset.get_max_length)\n",
    "# len(df[(df['max_length'] > 510)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1 = BertTokenizer.from_pretrained(\"indobenchmark/indobert-lite-base-p2\")\n",
    "dataset = EssayDataset(df, tokenizer1, 512)\n",
    "\n",
    "df['max_length1'] = df.index.map(dataset.get_max_length)\n",
    "len(df[(df['dataset'] != 'analisis_essay')&(df['max_length1'] > 510)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22406.000000\n",
       "mean       161.063688\n",
       "std        108.950303\n",
       "min         30.000000\n",
       "25%        114.000000\n",
       "50%        150.000000\n",
       "75%        186.000000\n",
       "max       1832.000000\n",
       "Name: max_length1, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['max_length1'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_score(row, min_max_dict):\n",
    "    min_score, max_score = min_max_dict[row['dataset']]\n",
    "    return int((row['score'] - min_score) / (max_score - min_score)*100)\n",
    "\n",
    "# Menghitung min dan max per kategori sebagai tuple\n",
    "min_max_dict = df.groupby('dataset')['score'].agg(['min', 'max']).apply(tuple, axis=1).to_dict()\n",
    "\n",
    "# Menambahkan kolom normalized_score\n",
    "df['normalized_score2'] = df.apply(lambda x: normalize_score(x, min_max_dict), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22406.000000\n",
       "mean        38.206552\n",
       "std         33.824810\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%         33.000000\n",
       "75%         66.000000\n",
       "max        100.000000\n",
       "Name: normalized_score2, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['normalized_score2'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2558 entries, 19376 to 21933\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   question           2558 non-null   object \n",
      " 1   reference_answer   2558 non-null   object \n",
      " 2   answer             2558 non-null   object \n",
      " 3   score              2558 non-null   float64\n",
      " 4   dataset            2558 non-null   object \n",
      " 5   max_length1        2558 non-null   int64  \n",
      " 6   normalized_score   2558 non-null   float64\n",
      " 7   normalized_score2  2558 non-null   int64  \n",
      "dtypes: float64(2), int64(2), object(4)\n",
      "memory usage: 179.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_sag = df[df['dataset'] == 'sag']\n",
    "df_sag.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12 entries, 19376 to 19387\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   question           12 non-null     object \n",
      " 1   reference_answer   12 non-null     object \n",
      " 2   answer             12 non-null     object \n",
      " 3   score              12 non-null     float64\n",
      " 4   dataset            12 non-null     object \n",
      " 5   max_length1        12 non-null     int64  \n",
      " 6   normalized_score   12 non-null     float64\n",
      " 7   normalized_score2  12 non-null     int64  \n",
      "dtypes: float64(2), int64(2), object(4)\n",
      "memory usage: 864.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_test = df_sag[:12]\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['max_length1'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = LongEssayDataset(test, tokenizer1, 512, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(test_dataset, batch_size=4, collate_fn=lambda x: list(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([{'input_ids': tensor([    2, 29791, 29950,  2293, 29835, 10948,  4012, 24045,     7, 29950,\n",
      "        20887, 17732,   374,   223,  5718,  1742, 26544, 20718,   133,    48,\n",
      "        19967, 14561,    10, 29946, 19998, 19425, 29840,  1002,  7012,    55,\n",
      "        27952, 29950,    11, 29942,    10, 29943, 29953,   111, 20741,   101,\n",
      "         1116,   918,  9689,  5707, 29946,    11, 29942,   253, 29943, 29953,\n",
      "          502,  1548, 20232,   630, 20741,   101,  1116, 29442, 29835,    11,\n",
      "        29942,   253, 29855,   193, 29856, 29838, 29948, 29948, 29943, 29953,\n",
      "           11, 29942,   253, 29855, 29943, 29945,    11, 29942,   253, 29856,\n",
      "        29943, 29948, 29948, 29948,  1548, 20232,   630, 27807,  2754,  1116,\n",
      "        17690, 29947,  1226, 29947, 17690, 28600,  7623,  9533, 19996, 29942,\n",
      "        20741,   101,  1116,  7063, 14383, 29840, 29943, 29950, 20422,    20,\n",
      "        20718,  7503, 29950,  1002, 20887, 17732,  1116,   223,  5901,   253,\n",
      "          374,  1002, 17260,  1709,  7801,     9,  1002, 13079,  1116, 13687,\n",
      "        29840, 18989,  4864,  1226,  3379, 29848,   253,   136,  1002,  2405,\n",
      "        13079,  1116,   598,   106,  9084, 13687, 29840, 29946,  3636,  3999,\n",
      "         2692,  8666, 29611,  5811, 12535,  1742, 29849,  9670,  1290,   223,\n",
      "         1669,   135,   591,  1116, 25492,  1116,  5379, 20718,  7503,   374,\n",
      "         1002, 16288,  4993,  2754,  1116,   253,  5816,  6706,   253, 26843,\n",
      "         1116,  2642, 22369,  2405, 29948,  4888,  3636,  4528,  8666,  4888,\n",
      "        16862,  1226,  1435, 14556,  1340,  8666,  1002,   373,  3410, 19429,\n",
      "        29840, 25272, 29840,  3009, 29948, 19252,  1290,  1002, 18989,  4864,\n",
      "        13687,   374,  6307,   136,  1002,   598,   106,  9084, 13687, 29840,\n",
      "         5811,  2642, 29948,  1002, 14765,  2523,  9104,  6019,   374, 21314,\n",
      "         8583, 29835,   111, 29949,  2642, 29948,  2786,  7046, 12006,   317,\n",
      "        20718,  7503, 29950,  1002,    11, 29942,   253, 29943,  5362,  1116,\n",
      "         5901,   253,   374,  1002, 17887,  1116,  1002, 19705,  2786,  7046,\n",
      "         5683, 29849,  4232, 19998,   253, 29534,  4421,    48,   253,  5634,\n",
      "        10356,  1116,  1271,  5249,   133,  4243, 29840,  7906,  1002,  1620,\n",
      "        29835, 27573, 29840,  1669,   135,   591, 29950,  1002, 20887, 17732,\n",
      "         1116,   253,  1713, 13752, 29833, 19502,  2692, 11055,  7640,   374,\n",
      "         1226,  1432,  1435, 14556,  6548, 29948,  3009,  1116,  1763, 29946,\n",
      "         9531, 24479, 29840, 21314, 26961,  6119, 29946,  4941, 11055, 17953,\n",
      "        29948,   294,  1002, 13079,  1116,  4243, 29840,   374,  7460,   934,\n",
      "         5392,  6861,   136, 20232,   630,  4243,   374, 23532,  1116,  1002,\n",
      "        17175, 29946,  4888, 17626,  1002,  2786,  7046, 12006,   317, 20718,\n",
      "         7503, 29948, 22899,   624,  1479, 13497,  1732, 29950,  1002, 20887,\n",
      "        17732,  1116,   223,  5901,   374,  1002, 11319,  9797,  1116, 21257,\n",
      "         8666,   223,  8690, 29946, 14226,   490,  1002, 16823,  1991,   598,\n",
      "         1045,  1045,   117,   253, 23251, 29833,  6199, 29946,  3636,  5352,\n",
      "        29840,  1226,  1002, 29534, 20478,  4012,  1116,  1002,  5901,   223,\n",
      "         1669,   135,   591,  1116,  5379,   374,  1002, 20887, 17732,  8666,\n",
      "          223, 28079, 12123, 10925,  7193,  1002, 19721, 29948,  1701, 10925,\n",
      "         1432,   253,  6855,   136, 22899,  3732, 22272,  1732, 29946, 14226,\n",
      "          490, 20940, 17894,  4855,  8666, 20232,   630,  1116,  1044,  4442,\n",
      "         6587, 29948,  9315,    48,  2074, 29836,  4888, 11176,  8666,  1002,\n",
      "        28079, 12123,  5243,  6459,   133, 20076, 16742,   136,   374, 28560,\n",
      "         3437,  1290, 24926, 29946, 24008, 15156, 11500, 29835,   253,  7460,\n",
      "        20887, 17732, 12028,  1226, 10198,  7193,  7467, 29840, 29948,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}, {'input_ids': tensor([    3, 16608, 24045,     7, 29950, 20887, 17732,   374,   253, 26544,\n",
      "        20718,   133,  4232,    17,  1002, 19967, 14561,   136,  5243, 18211,\n",
      "         2724, 20741,   101, 29840, 29950,  1002, 20741,   101,  1116,   918,\n",
      "         9689,  5707, 28645,    11, 29942,    10, 29943, 29953,   111,   374,\n",
      "        15783,  6548,  4232,  1002,   918,  9689,  5901,   136,   502,  4232,\n",
      "         1002,  3839,  4915, 19996,  5901, 29951,  1002, 20741,   101,  1116,\n",
      "        29442,  1479, 28645,    11, 29942,   253, 29943, 29953,   502,  1548,\n",
      "        20232,   630,   253, 28645,  1002, 20887, 17732,   374,  1435, 14556,\n",
      "         6548,  8666,  1002,  5901,   374,  1709,  7801,     9,   502,   136,\n",
      "          111, 29951,   136,  5638,  1290,  1002, 20741,   101,  1116,  1002,\n",
      "         7063, 14383, 28645,    11, 29942,   373, 29855,   489,   373, 29856,\n",
      "        29943, 29953,    11, 29942,   373, 29855, 29943, 29944,    11, 29942,\n",
      "          373, 29856, 29943,  1548, 20232,   630,    35,  7046,  5683, 29835,\n",
      "         1116, 28600,    48,  1002, 19967, 14561, 29948,  6706,  1002,  1619,\n",
      "          175, 20741,   101, 23039, 23983,    53,  1002, 24366,   591,  1116,\n",
      "         2405, 20887, 17732, 19998,   374, 12535,    20,  1226,  1002, 10521,\n",
      "         1116,  1002,  8690, 20887, 17732,  1116, 28600, 12657,  1002,   613,\n",
      "        17307,  2173,  1709,  7801,     9,  1002, 17690, 29948, 20113,  1701,\n",
      "        25272, 29840,  1226, 20887, 17732,  4888,  4442, 23039, 10948,  1226,\n",
      "        25326,   153, 13497,  8225, 29950,  1002, 20422,    20, 20718,  7503,\n",
      "        28645,  1002, 20887, 17732,  1116, 29534, 20478,  4012,  1116,  1002,\n",
      "         5901,   374, 12535,    20,  1226,  1002, 17260,  1116,  1002, 13079,\n",
      "         1116, 18989,  4864, 13687, 29840,  1226,  1002, 13079,  1116,   598,\n",
      "          106,  9084, 13687, 29840, 18023,  8552,   133,  8666, 21039,  5811,\n",
      "        12535,  1742, 29849,  9670,  1290, 29942,   223,  1669,   135,   591,\n",
      "         2481,  1432,  1002, 11097,   630,  1116,   253, 20975, 29842,  5630,\n",
      "         3113, 18274, 28645,  3299, 16862,  1701,  1226,  4243,  1002, 13079,\n",
      "         1116, 18989,  4864, 13687, 29840,   374,   111,  2691,  6314,  1002,\n",
      "        13079,  1116,   598,   106,  9084, 13687, 29840,   374, 17690, 20113,\n",
      "         1002, 20887, 17732,  1116, 29534, 20478,  4012,  1116,  1002,  5901,\n",
      "        11479, 15370,  1432,   111, 29949,   109,   569,  1548,  1669,   135,\n",
      "          591,    48,  1002, 11097,   630,  1116, 17690,  1100, 29835, 28645,\n",
      "         3299, 15570,  4287,   598,   106,  9084, 13633,  4643,  2754, 29840,\n",
      "          136,  3299, 16862,  1002,   671,  1116,  1002, 17690,  1100, 29835,\n",
      "        12535,    20,  1226, 23358,    48,  5379, 13687,  1002, 20887, 17732,\n",
      "        10925,  1432, 23251, 29833,  2102,   418, 29949,  4287,   294,   418,\n",
      "         5811,  1002, 18989,  4864, 13687, 29840,   136,  4287,   598,   106,\n",
      "         9084,  6307, 29840, 29948,  1002,  9757, 20718,  7503,   374,  8666,\n",
      "         2786,  7046, 12006,   317,  1782,  4642,  1002, 20887, 17732,  1116,\n",
      "         1002, 29534, 20478,  4012,  1116,  1002,  5901,   374,  1002, 17887,\n",
      "         1116,  1002,  2786,  7046,  5683, 29849, 19705,  1226,  1002, 29534,\n",
      "        20478,  4012,  1116,  1002,  5901,    48,   253, 27807,  2754,  1116,\n",
      "        23532,  4243, 29840,  2035, 12717, 29842,  3009,  7906,  1002,  1620,\n",
      "        29835, 27573, 29840, 29942,  1548,  1669,   135,   591,   136,  3299,\n",
      "        16862,  1226,  1435, 14556,  1340,  1002,  2074, 29836,  8666,  1701,\n",
      "        10925, 20669, 17355, 29835,    48,   253, 23251, 29833, 15570,   136,\n",
      "         1701,   374, 11176, 29833,  8666,    48,  1002, 12622,  1558, 18885,\n",
      "         1701,  5243, 20482,   133,  1233, 16642,  1002, 20887, 17732,  8666,\n",
      "         1701, 10925, 20669, 17355, 29835,    48,  8666, 15570,   374, 12535,\n",
      "           20,  1226,  1233, 29949,  1558, 29943, 29951,  5638,  1290,  1002,\n",
      "        28126, 29842, 13497,  1732,   374, 23251, 29833,  2102,  1002, 20617,\n",
      "         3732,     3]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'token_type_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}, {'input_ids': tensor([    3,  4642,  1002, 20887, 17732,  1116,  1002, 29534, 20478,  4012,\n",
      "         1116,  1002,  5901,   374,  1002, 17887,  1116,  1002,  2786,  7046,\n",
      "         5683, 29849, 19705,  1226,  1002, 29534, 20478,  4012,  1116,  1002,\n",
      "         5901,    48,   253, 27807,  2754,  1116, 23532,  4243, 29840,  2035,\n",
      "        12717, 29842,  3009,  7906,  1002,  1620, 29835, 27573, 29840, 29942,\n",
      "         1548,  1669,   135,   591,   136,  3299, 16862,  1226,  1435, 14556,\n",
      "         1340,  1002,  2074, 29836,  8666,  1701, 10925, 20669, 17355, 29835,\n",
      "           48,   253, 23251, 29833, 15570,   136,  1701,   374, 11176, 29833,\n",
      "         8666,    48,  1002, 12622,  1558, 18885,  1701,  5243, 20482,   133,\n",
      "         1233, 16642,  1002, 20887, 17732,  8666,  1701, 10925, 20669, 17355,\n",
      "        29835,    48,  8666, 15570,   374, 12535,    20,  1226,  1233, 29949,\n",
      "         1558, 29943, 29951,  5638,  1290,  1002, 28126, 29842, 13497,  1732,\n",
      "          374, 23251, 29833,  2102,  1002, 20617,  3732,   317, 20718,  7503,\n",
      "        29951, 14218,  1002, 20887, 17732,   374,  1002, 11319,  9797,  1116,\n",
      "        14983, 29271,  8666,   223,  8690, 29946,  6227, 12548,    55,  1226,\n",
      "        10198, 14406, 16823, 29946,  3636,  5352, 29840,  1226,  1002, 29534,\n",
      "        20478,  4012,  1116,  1002,  5901, 29948,   223,  1669,   135,   591,\n",
      "         2481,  1432,  1432,  1709, 29840, 29951,  8666,   374, 11088, 16742,\n",
      "          223,  8690,   374, 10925,    55,  1226,  2403,  1226,  6085,    31,\n",
      "           48, 13687,  1116,  7193,  7467,   136,  3756, 18112,    48, 13687,\n",
      "         1116, 19308, 29948,  9315,  1002,  5901,   374,  3839,  4915, 19996,\n",
      "         3299, 10925,  3756,  1432, 10925,    55,  1226, 24461,     7, 16467,\n",
      "         9315,  1002,  2403,  2187,   374,  7460, 29946,  9315,  1002,  5901,\n",
      "          374,    78,  1192, 13417,  1226,  1709, 12220,  9315,  1002,  2403,\n",
      "         2187,   374,  7460, 29946,  9315,  1701,   374, 22616, 29849,  9670,\n",
      "         1290,  3299, 10925, 24461,     7, 16467,  9315,  1002,  2403,  2187,\n",
      "          374,  8765,  2691,  6314,  9315,  1701,   374, 25425,   894,  6307,\n",
      "        10925,  1432, 10925,    55,  1226, 24461,     7, 26733,   764, 29948,\n",
      "         1701,   374, 23039, 27518,  1154,  3700,  8666, 25788, 29840,  4442,\n",
      "         1432,  3913, 15480, 29842,  1226,   934,  2026, 29835,  8666, 21039,\n",
      "         5811,  9911,  1709, 29840, 29948,     3,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}], [{'input_ids': tensor([    2, 29791, 29950,  2293, 29835, 10948,  4012, 24045,     7, 29950,\n",
      "        20887, 17732,   374,   223,  5718,  1742, 26544, 20718,   133,    48,\n",
      "        19967, 14561,    10, 29946, 19998, 19425, 29840,  1002,  7012,    55,\n",
      "        27952, 29950,    11, 29942,    10, 29943, 29953,   111, 20741,   101,\n",
      "         1116,   918,  9689,  5707, 29946,    11, 29942,   253, 29943, 29953,\n",
      "          502,  1548, 20232,   630, 20741,   101,  1116, 29442, 29835,    11,\n",
      "        29942,   253, 29855,   193, 29856, 29838, 29948, 29948, 29943, 29953,\n",
      "           11, 29942,   253, 29855, 29943, 29945,    11, 29942,   253, 29856,\n",
      "        29943, 29948, 29948, 29948,  1548, 20232,   630, 27807,  2754,  1116,\n",
      "        17690, 29947,  1226, 29947, 17690, 28600,  7623,  9533, 19996, 29942,\n",
      "        20741,   101,  1116,  7063, 14383, 29840, 29943, 29950, 20422,    20,\n",
      "        20718,  7503, 29950,  1002, 20887, 17732,  1116,   223,  5901,   253,\n",
      "          374,  1002, 17260,  1709,  7801,     9,  1002, 13079,  1116, 13687,\n",
      "        29840, 18989,  4864,  1226,  3379, 29848,   253,   136,  1002,  2405,\n",
      "        13079,  1116,   598,   106,  9084, 13687, 29840, 29946,  3636,  3999,\n",
      "         2692,  8666, 29611,  5811, 12535,  1742, 29849,  9670,  1290,   223,\n",
      "         1669,   135,   591,  1116, 25492,  1116,  5379, 20718,  7503,   374,\n",
      "         1002, 16288,  4993,  2754,  1116,   253,  5816,  6706,   253, 26843,\n",
      "         1116,  2642, 22369,  2405, 29948,  4888,  3636,  4528,  8666,  4888,\n",
      "        16862,  1226,  1435, 14556,  1340,  8666,  1002,   373,  3410, 19429,\n",
      "        29840, 25272, 29840,  3009, 29948, 19252,  1290,  1002, 18989,  4864,\n",
      "        13687,   374,  6307,   136,  1002,   598,   106,  9084, 13687, 29840,\n",
      "         5811,  2642, 29948,  1002, 14765,  2523,  9104,  6019,   374, 21314,\n",
      "         8583, 29835,   111, 29949,  2642, 29948,  2786,  7046, 12006,   317,\n",
      "        20718,  7503, 29950,  1002,    11, 29942,   253, 29943,  5362,  1116,\n",
      "         5901,   253,   374,  1002, 17887,  1116,  1002, 19705,  2786,  7046,\n",
      "         5683, 29849,  4232, 19998,   253, 29534,  4421,    48,   253,  5634,\n",
      "        10356,  1116,  1271,  5249,   133,  4243, 29840,  7906,  1002,  1620,\n",
      "        29835, 27573, 29840,  1669,   135,   591, 29950,  1002, 20887, 17732,\n",
      "         1116,   253,  1713, 13752, 29833, 19502,  2692, 11055,  7640,   374,\n",
      "         1226,  1432,  1435, 14556,  6548, 29948,  3009,  1116,  1763, 29946,\n",
      "         9531, 24479, 29840, 21314, 26961,  6119, 29946,  4941, 11055, 17953,\n",
      "        29948,   294,  1002, 13079,  1116,  4243, 29840,   374,  7460,   934,\n",
      "         5392,  6861,   136, 20232,   630,  4243,   374, 23532,  1116,  1002,\n",
      "        17175, 29946,  4888, 17626,  1002,  2786,  7046, 12006,   317, 20718,\n",
      "         7503, 29948, 22899,   624,  1479, 13497,  1732, 29950,  1002, 20887,\n",
      "        17732,  1116,   223,  5901,   374,  1002, 11319,  9797,  1116, 21257,\n",
      "         8666,   223,  8690, 29946, 14226,   490,  1002, 16823,  1991,   598,\n",
      "         1045,  1045,   117,   253, 23251, 29833,  6199, 29946,  3636,  5352,\n",
      "        29840,  1226,  1002, 29534, 20478,  4012,  1116,  1002,  5901,   223,\n",
      "         1669,   135,   591,  1116,  5379,   374,  1002, 20887, 17732,  8666,\n",
      "          223, 28079, 12123, 10925,  7193,  1002, 19721, 29948,  1701, 10925,\n",
      "         1432,   253,  6855,   136, 22899,  3732, 22272,  1732, 29946, 14226,\n",
      "          490, 20940, 17894,  4855,  8666, 20232,   630,  1116,  1044,  4442,\n",
      "         6587, 29948,  9315,    48,  2074, 29836,  4888, 11176,  8666,  1002,\n",
      "        28079, 12123,  5243,  6459,   133, 20076, 16742,   136,   374, 28560,\n",
      "         3437,  1290, 24926, 29946, 24008, 15156, 11500, 29835,   253,  7460,\n",
      "        20887, 17732, 12028,  1226, 10198,  7193,  7467, 29840, 29948,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}, {'input_ids': tensor([    3, 16608, 24045,     7, 29950,  1002, 20887, 17732,   374,   253,\n",
      "         6525, 29844,  1116, 21871,  1999,  7291,  8666,  1435, 14556, 17142,\n",
      "         1002, 18383,  1169,   326, 29849,  1116,  1002, 29534, 20478,  4012,\n",
      "         1116,   223,  5901, 29948,  1226,  1432, 20718,   133,   294, 15290,\n",
      "        29847, 29946,  1701,  8799,  5489,  3260, 20741,   101, 29840, 19998,\n",
      "         5811,  1002, 20741,   101,  1116,   918,  9689,  5707, 29946, 29442,\n",
      "         1479,   136,  7063, 14383, 29840, 29948,  1002, 20741,   101,  1116,\n",
      "          918,  9689,  5707, 21707, 29840,  1002, 29534, 20478,  4012,  1116,\n",
      "         1002,  5901,    48,   253,   918,  9689,  9789, 29948,  1002, 20741,\n",
      "          101,  1116, 29442,  1479, 21707, 29840,  8666,  1002, 20887, 17732,\n",
      "          374, 29024, 12535,    20,  1226,   569, 13332,     7, 18241, 22001,\n",
      "        29948,  2102, 20741,   101,  1116,  1002,  7063, 14383,  1701,   374,\n",
      "         1666, 24812,  8666,  1002, 20887, 17732,  1116, 13633, 15026, 17690,\n",
      "        28600,   374, 23251, 29833,  2102,  1002,   671,  1116,  1002, 20887,\n",
      "        17732,  1116,  1002, 10806,  5901,  4232,  1002, 20887, 17732,  1116,\n",
      "         1002,  9757,  5901, 29948, 23251, 29833,  1002, 20741,  2741,  3871,\n",
      "        20718,  7503, 29946, 21314,  5811, 24008, 18211,  4339, 29840,  1226,\n",
      "        13497,  1732,  1116, 20887, 17732, 29950, 20422,    20,   569, 20617,\n",
      "         3732, 13497,  1732, 29946,  2786,  7046, 12006,   317,   136, 20617,\n",
      "         3732,   317, 29948, 20422,    20, 20887, 17732,   374, 15290, 29847,\n",
      "        20113,  1002,  8218,  1116, 20887, 17732,  1435, 14556,  1732, 11152,\n",
      "           28, 29853,   374,  1002, 23425, 19221,  4864, 29946,    89, 29948,\n",
      "          373, 29948, 26647,  3700, 28600,  5811, 12535,  1742, 29849,  9670,\n",
      "         1290,  1226, 29534,    58, 29948,  1548,  1669,   135,   591, 29946,\n",
      "         9315,  3299, 16862,  1226,  1435, 14556,  1340,  1002, 29941,  7210,\n",
      "         2187,  1116,  1002, 18211, 29941,  5901,  2102, 13788,  4842,  2692,\n",
      "          253,  2293, 29947, 23358, 29947, 15842, 29842, 19390, 29946,  1002,\n",
      "        20422, 20887, 17732,  1116,  1002, 28692,  1116,  1002, 29339, 12832,\n",
      "        13079,   374,  6307, 29947, 23358,  1469, 29948,  2786,  7046, 12006,\n",
      "        20887, 17732, 25272, 29840, 26120,  4202, 20113,  3299,  5811,  3756,\n",
      "          574, 29835,  1116,  1002, 12535, 17665,   153, 29946,   391,  3299,\n",
      "        12601,  1226,  1782,   253, 13079,  1116,  4243, 29840,  8666,  9204,\n",
      "         1226, 26465,  1479, 29946,  8987,    55,  1002, 27807,  2786,  7046,\n",
      "         5683, 29849,  1116,  1002, 29339, 12832,  5901, 29948,  9315, 29946,\n",
      "         1548,  1669,   135,   591, 29946,  3299, 16862,  1226,  1435, 14556,\n",
      "         1340,  1002, 20887, 17732,  1116,  1563,    55,  3009,  5674,    48,\n",
      "          223,    78,  2689, 21552,  5630,  3113, 18274, 29946,  3299, 10925,\n",
      "        12601,  1226,  6587,   253,  7460, 13079,  1116, 13788,  2433,   136,\n",
      "        10574,  1002, 27807,  1116,  1002, 29941,  5674, 29941,  5901, 29948,\n",
      "        22899,  3732, 20887, 17732,   374, 25508, 20113,  1701,   374,  3756,\n",
      "          598,   106,  9084,  1226, 12601,   918,  9689,  1006,   490,  1002,\n",
      "        27807,  1116,   223,  5901, 29948,  1002,  1435, 14556,  1732,  1116,\n",
      "         1002, 20617,  3732,   317, 29946, 23251, 29833,  1002, 20718,  7503,\n",
      "         1116,  1595,  4182, 26370, 29946, 16384, 29840, 15030,  2102, 11319,\n",
      "         9797, 29840,  1116,  1320,  1083,  1116,   253, 17260,   491, 29844,\n",
      "         8690, 11075,   374, 14226,   490,  1002, 23128,   136, 11951,  1116,\n",
      "         1002, 27807,  1116,  1002,  5901, 29948,   223,  5778,  1669,   135,\n",
      "          591,   374,  1002,  1709, 29840, 29946,   490, 19998,  3299,  4442,\n",
      "         1563,  1226,  1435, 14556,  1340,  1002, 11319,  9797,  1116,  1320,\n",
      "         1083, 29948,  9315,  3299, 16862,  1226,  6587,   253,  1709, 28645,\n",
      "         3299,   840, 19900,   136,  3299,  7193, 11088, 16742,  3299,  5811,\n",
      "        23251,     3]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'token_type_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}, {'input_ids': tensor([    3, 29941,  5674, 29941,  5901, 29948, 22899,  3732, 20887, 17732,\n",
      "          374, 25508, 20113,  1701,   374,  3756,   598,   106,  9084,  1226,\n",
      "        12601,   918,  9689,  1006,   490,  1002, 27807,  1116,   223,  5901,\n",
      "        29948,  1002,  1435, 14556,  1732,  1116,  1002, 20617,  3732,   317,\n",
      "        29946, 23251, 29833,  1002, 20718,  7503,  1116,  1595,  4182, 26370,\n",
      "        29946, 16384, 29840, 15030,  2102, 11319,  9797, 29840,  1116,  1320,\n",
      "         1083,  1116,   253, 17260,   491, 29844,  8690, 11075,   374, 14226,\n",
      "          490,  1002, 23128,   136, 11951,  1116,  1002, 27807,  1116,  1002,\n",
      "         5901, 29948,   223,  5778,  1669,   135,   591,   374,  1002,  1709,\n",
      "        29840, 29946,   490, 19998,  3299,  4442,  1563,  1226,  1435, 14556,\n",
      "         1340,  1002, 11319,  9797,  1116,  1320,  1083, 29948,  9315,  3299,\n",
      "        16862,  1226,  6587,   253,  1709, 28645,  3299,   840, 19900,   136,\n",
      "         3299,  7193, 11088, 16742,  3299,  5811, 23251, 29833,  4119,  6307,\n",
      "        29946,  3299, 10925,  1435, 14556,  1340,  1002, 20887, 17732,  2102,\n",
      "         4338,  9618, 29843,  1002,   622, 28219, 23251, 29833,  2102,  1002,\n",
      "          671, 23251, 29833,  4119,  1002,  7193, 29948,     3,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}], [{'input_ids': tensor([    2, 29791, 29950,  2293, 29835, 10948,  4012, 24045,     7, 29950,\n",
      "        20887, 17732,   374,   223,  5718,  1742, 26544, 20718,   133,    48,\n",
      "        19967, 14561,    10, 29946, 19998, 19425, 29840,  1002,  7012,    55,\n",
      "        27952, 29950,    11, 29942,    10, 29943, 29953,   111, 20741,   101,\n",
      "         1116,   918,  9689,  5707, 29946,    11, 29942,   253, 29943, 29953,\n",
      "          502,  1548, 20232,   630, 20741,   101,  1116, 29442, 29835,    11,\n",
      "        29942,   253, 29855,   193, 29856, 29838, 29948, 29948, 29943, 29953,\n",
      "           11, 29942,   253, 29855, 29943, 29945,    11, 29942,   253, 29856,\n",
      "        29943, 29948, 29948, 29948,  1548, 20232,   630, 27807,  2754,  1116,\n",
      "        17690, 29947,  1226, 29947, 17690, 28600,  7623,  9533, 19996, 29942,\n",
      "        20741,   101,  1116,  7063, 14383, 29840, 29943, 29950, 20422,    20,\n",
      "        20718,  7503, 29950,  1002, 20887, 17732,  1116,   223,  5901,   253,\n",
      "          374,  1002, 17260,  1709,  7801,     9,  1002, 13079,  1116, 13687,\n",
      "        29840, 18989,  4864,  1226,  3379, 29848,   253,   136,  1002,  2405,\n",
      "        13079,  1116,   598,   106,  9084, 13687, 29840, 29946,  3636,  3999,\n",
      "         2692,  8666, 29611,  5811, 12535,  1742, 29849,  9670,  1290,   223,\n",
      "         1669,   135,   591,  1116, 25492,  1116,  5379, 20718,  7503,   374,\n",
      "         1002, 16288,  4993,  2754,  1116,   253,  5816,  6706,   253, 26843,\n",
      "         1116,  2642, 22369,  2405, 29948,  4888,  3636,  4528,  8666,  4888,\n",
      "        16862,  1226,  1435, 14556,  1340,  8666,  1002,   373,  3410, 19429,\n",
      "        29840, 25272, 29840,  3009, 29948, 19252,  1290,  1002, 18989,  4864,\n",
      "        13687,   374,  6307,   136,  1002,   598,   106,  9084, 13687, 29840,\n",
      "         5811,  2642, 29948,  1002, 14765,  2523,  9104,  6019,   374, 21314,\n",
      "         8583, 29835,   111, 29949,  2642, 29948,  2786,  7046, 12006,   317,\n",
      "        20718,  7503, 29950,  1002,    11, 29942,   253, 29943,  5362,  1116,\n",
      "         5901,   253,   374,  1002, 17887,  1116,  1002, 19705,  2786,  7046,\n",
      "         5683, 29849,  4232, 19998,   253, 29534,  4421,    48,   253,  5634,\n",
      "        10356,  1116,  1271,  5249,   133,  4243, 29840,  7906,  1002,  1620,\n",
      "        29835, 27573, 29840,  1669,   135,   591, 29950,  1002, 20887, 17732,\n",
      "         1116,   253,  1713, 13752, 29833, 19502,  2692, 11055,  7640,   374,\n",
      "         1226,  1432,  1435, 14556,  6548, 29948,  3009,  1116,  1763, 29946,\n",
      "         9531, 24479, 29840, 21314, 26961,  6119, 29946,  4941, 11055, 17953,\n",
      "        29948,   294,  1002, 13079,  1116,  4243, 29840,   374,  7460,   934,\n",
      "         5392,  6861,   136, 20232,   630,  4243,   374, 23532,  1116,  1002,\n",
      "        17175, 29946,  4888, 17626,  1002,  2786,  7046, 12006,   317, 20718,\n",
      "         7503, 29948, 22899,   624,  1479, 13497,  1732, 29950,  1002, 20887,\n",
      "        17732,  1116,   223,  5901,   374,  1002, 11319,  9797,  1116, 21257,\n",
      "         8666,   223,  8690, 29946, 14226,   490,  1002, 16823,  1991,   598,\n",
      "         1045,  1045,   117,   253, 23251, 29833,  6199, 29946,  3636,  5352,\n",
      "        29840,  1226,  1002, 29534, 20478,  4012,  1116,  1002,  5901,   223,\n",
      "         1669,   135,   591,  1116,  5379,   374,  1002, 20887, 17732,  8666,\n",
      "          223, 28079, 12123, 10925,  7193,  1002, 19721, 29948,  1701, 10925,\n",
      "         1432,   253,  6855,   136, 22899,  3732, 22272,  1732, 29946, 14226,\n",
      "          490, 20940, 17894,  4855,  8666, 20232,   630,  1116,  1044,  4442,\n",
      "         6587, 29948,  9315,    48,  2074, 29836,  4888, 11176,  8666,  1002,\n",
      "        28079, 12123,  5243,  6459,   133, 20076, 16742,   136,   374, 28560,\n",
      "         3437,  1290, 24926, 29946, 24008, 15156, 11500, 29835,   253,  7460,\n",
      "        20887, 17732, 12028,  1226, 10198,  7193,  7467, 29840, 29948,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}, {'input_ids': tensor([    3, 16608, 24045,     7, 29950,  1002, 20741,  2741,  3871, 20887,\n",
      "        17732,   612,  9071,  1044, 12444,  5811,  1002, 20741,   101, 29840,\n",
      "          569,  2162, 14657,    53,  8666,  8799,  1432,  5489, 17574,  1548,\n",
      "         1002,   848, 27443, 29835,  1226,  1432, 20718,  4864,   294, 20887,\n",
      "        17732, 29948,  1002, 20887, 17732,   374,   223,  5718,  1742, 26544,\n",
      "        20718,   133,    48,  1002,    10, 19967, 14561, 29946, 19998,  5243,\n",
      "         1002,  7012,    55, 27952, 29950, 29947,    11, 29942,    10, 29943,\n",
      "        29953,   111, 29942, 20741,   101,   918,  9689,  5707, 29943, 29951,\n",
      "        29947,    11, 29942,   253, 29943, 29954, 29953,   502, 29948,    62,\n",
      "          253, 29942, 20741,   101,  1116, 29442,  1479, 29943, 29951, 29947,\n",
      "           11, 29942,   253, 29855,   489,   253, 29856,   489, 29948, 29948,\n",
      "        29948, 29943, 29953,    11, 29942,   253, 29855, 29943, 29945,    11,\n",
      "        29942,   253, 29856, 29943, 29945, 29948, 29948, 29948,  1548, 20232,\n",
      "          630, 27807,  2754,  1116, 17690, 29947,  1226, 29947, 17690, 28600,\n",
      "         7623,  9533, 19996, 29942, 21852,  1116,  1002,  7063, 14383, 29943,\n",
      "         5379, 13497,  1732,  9211,    48, 18211, 17175, 10991,    53,  8666,\n",
      "        19120,  1226, 25326,   153,   818,  8225, 29950, 29947, 20422,    20,\n",
      "        29942, 20617,  3732, 29943, 13497,  1732, 29950,  1002, 20887, 17732,\n",
      "         1116,   223,  5901,   253,   374,  1002, 17260,  1709,  7801,     9,\n",
      "         1002, 13079,  1116, 13687, 29840, 18989,  4864,  1226,  1002, 29534,\n",
      "        20478,  4012,  1116,   253,   136,  1002,  2405, 13079,  1116,   598,\n",
      "          106,  9084, 13687, 29840, 29946,  3636,  3999,  2692,  8666, 29611,\n",
      "         5811, 12535,  1742, 29849,  9670,  1290, 29942,  2743, 26647,  3700,\n",
      "        28600, 12601,  1002,  1620, 29835, 20887, 17732, 29943, 29948,  1701,\n",
      "        10841,  1290, 15355,  5953, 28645, 21314,  5811, 21871,  1999,  5039,\n",
      "        26231, 12651,   254,   309,  1226,  1435, 14556,  1340,  5379, 20887,\n",
      "        17732, 29948,  4888,  4442, 16384,  1002,  1669,   135,   591,  4232,\n",
      "          253, 20975, 29842, 28449, 29946,    89, 12601,  1002,  1620, 29835,\n",
      "        20887, 17732,  1548,  3805, 29847, 18793, 29840,  1116,  1002,   111,\n",
      "        29949,   109, 28449, 29948, 29947,  2786,  7046, 12006,   317, 13497,\n",
      "         1732, 29942,  2357,   341, 29943, 29950,  1002,    11, 29942,   253,\n",
      "        29943, 20887, 17732,  1116,  5901,   253,   374,  1002, 17887,  1116,\n",
      "         1002, 19705,  2786,  7046,  5683, 29849,  4232, 19998,   253, 29534,\n",
      "         4421,    48,   253,  5634, 10356,  1116,  4243, 29840,  7744,  7513,\n",
      "          133,  7906,  1002,  1620, 29835, 27573, 29840, 29948,  1226,  1980,\n",
      "         9315,  3717, 28449,   374, 20975, 29842,   569,  3756, 29946,    89,\n",
      "        29941,     8,  1563,    55,  1226,  1782,   253, 10356,  1116,  1271,\n",
      "         5249,   133,  4243, 29840,  7906,  1002,  1620, 29835, 27573, 29840,\n",
      "        29948,    89, 13993,   253,  6119, 29947,  6542, 29842,  2786,  7046,\n",
      "         5683, 29849,  1116,  5674,   490,   253, 10356,  1116,  1763,  4243,\n",
      "        29840, 29950,  3717, 19705,  2786,  7046,  5683, 29849, 10925,  1432,\n",
      "          502, 29948,  6119, 27898,  7296, 12870, 29835,  1226,  1002, 20887,\n",
      "        17732,  1116,  5674, 29534, 20478,    55, 29948,  1002, 20887, 17732,\n",
      "          374, 23251, 29833,  2102,  1002, 19705,  2786,  7046,  5683, 29849,\n",
      "        29948, 29947, 20617,  3732,   317, 13497,  1732, 29950,  1002, 20887,\n",
      "        17732,  1116,   223,  5901,   374,  1002, 11319,  9797,  1116, 14983,\n",
      "        29271,  8666,   223,  8690, 29946,   490,  1002,  8218,  1116, 16823,\n",
      "          598,  1045, 11753,   117,   253, 23251, 29833,  6199, 29946,  3636,\n",
      "         5352, 29840,  1002, 29534, 20478,  4012,  1116,  1002,  5901, 29948,\n",
      "         1701,   374, 21314,  8583, 29835, 14226,   490,  1002, 11319,  9797,\n",
      "         1116, 14983, 29271,  1116,  1002, 17260,   491, 29844, 22899, 11075,\n",
      "        29946,     3]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'token_type_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}, {'input_ids': tensor([    3, 29947,  6542, 29842,  2786,  7046,  5683, 29849,  1116,  5674,\n",
      "          490,   253, 10356,  1116,  1763,  4243, 29840, 29950,  3717, 19705,\n",
      "         2786,  7046,  5683, 29849, 10925,  1432,   502, 29948,  6119, 27898,\n",
      "         7296, 12870, 29835,  1226,  1002, 20887, 17732,  1116,  5674, 29534,\n",
      "        20478,    55, 29948,  1002, 20887, 17732,   374, 23251, 29833,  2102,\n",
      "         1002, 19705,  2786,  7046,  5683, 29849, 29948, 29947, 20617,  3732,\n",
      "          317, 13497,  1732, 29950,  1002, 20887, 17732,  1116,   223,  5901,\n",
      "          374,  1002, 11319,  9797,  1116, 14983, 29271,  8666,   223,  8690,\n",
      "        29946,   490,  1002,  8218,  1116, 16823,   598,  1045, 11753,   117,\n",
      "          253, 23251, 29833,  6199, 29946,  3636,  5352, 29840,  1002, 29534,\n",
      "        20478,  4012,  1116,  1002,  5901, 29948,  1701,   374, 21314,  8583,\n",
      "        29835, 14226,   490,  1002, 11319,  9797,  1116, 14983, 29271,  1116,\n",
      "         1002, 17260,   491, 29844, 22899, 11075, 29946,  6227, 12548,    55,\n",
      "         1226, 10198, 16823, 29946, 20718,    53,  1002, 11319,  9797,  1116,\n",
      "        20887, 17732,  1116,   223,  5901, 29534, 20478,    55, 29948,   223,\n",
      "         1669,   135,   591,   374,  1002,  1709,   490,   253, 20753, 18193,\n",
      "        29946, 19998,   374, 14226,   490, 22899,  3732,  6958,  9824, 29946,\n",
      "           89, 29948,   373, 29948, 17260,   491, 29844,  8690, 29840,   136,\n",
      "        24498, 16823, 29946, 19998,  4442, 23039,  1432, 10842,  6593, 15621,\n",
      "        29840, 29946,  2691, 24062,  8981, 29840,  5811, 22427, 29839,   569,\n",
      "         6593, 27423,  7503, 29948,     3,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}], [{'input_ids': tensor([    2, 29791, 29950,  2293, 29835, 10948,  4012, 24045,     7, 29950,\n",
      "        20887, 17732,   374,   223,  5718,  1742, 26544, 20718,   133,    48,\n",
      "        19967, 14561,    10, 29946, 19998, 19425, 29840,  1002,  7012,    55,\n",
      "        27952, 29950,    11, 29942,    10, 29943, 29953,   111, 20741,   101,\n",
      "         1116,   918,  9689,  5707, 29946,    11, 29942,   253, 29943, 29953,\n",
      "          502,  1548, 20232,   630, 20741,   101,  1116, 29442, 29835,    11,\n",
      "        29942,   253, 29855,   193, 29856, 29838, 29948, 29948, 29943, 29953,\n",
      "           11, 29942,   253, 29855, 29943, 29945,    11, 29942,   253, 29856,\n",
      "        29943, 29948, 29948, 29948,  1548, 20232,   630, 27807,  2754,  1116,\n",
      "        17690, 29947,  1226, 29947, 17690, 28600,  7623,  9533, 19996, 29942,\n",
      "        20741,   101,  1116,  7063, 14383, 29840, 29943, 29950, 20422,    20,\n",
      "        20718,  7503, 29950,  1002, 20887, 17732,  1116,   223,  5901,   253,\n",
      "          374,  1002, 17260,  1709,  7801,     9,  1002, 13079,  1116, 13687,\n",
      "        29840, 18989,  4864,  1226,  3379, 29848,   253,   136,  1002,  2405,\n",
      "        13079,  1116,   598,   106,  9084, 13687, 29840, 29946,  3636,  3999,\n",
      "         2692,  8666, 29611,  5811, 12535,  1742, 29849,  9670,  1290,   223,\n",
      "         1669,   135,   591,  1116, 25492,  1116,  5379, 20718,  7503,   374,\n",
      "         1002, 16288,  4993,  2754,  1116,   253,  5816,  6706,   253, 26843,\n",
      "         1116,  2642, 22369,  2405, 29948,  4888,  3636,  4528,  8666,  4888,\n",
      "        16862,  1226,  1435, 14556,  1340,  8666,  1002,   373,  3410, 19429,\n",
      "        29840, 25272, 29840,  3009, 29948, 19252,  1290,  1002, 18989,  4864,\n",
      "        13687,   374,  6307,   136,  1002,   598,   106,  9084, 13687, 29840,\n",
      "         5811,  2642, 29948,  1002, 14765,  2523,  9104,  6019,   374, 21314,\n",
      "         8583, 29835,   111, 29949,  2642, 29948,  2786,  7046, 12006,   317,\n",
      "        20718,  7503, 29950,  1002,    11, 29942,   253, 29943,  5362,  1116,\n",
      "         5901,   253,   374,  1002, 17887,  1116,  1002, 19705,  2786,  7046,\n",
      "         5683, 29849,  4232, 19998,   253, 29534,  4421,    48,   253,  5634,\n",
      "        10356,  1116,  1271,  5249,   133,  4243, 29840,  7906,  1002,  1620,\n",
      "        29835, 27573, 29840,  1669,   135,   591, 29950,  1002, 20887, 17732,\n",
      "         1116,   253,  1713, 13752, 29833, 19502,  2692, 11055,  7640,   374,\n",
      "         1226,  1432,  1435, 14556,  6548, 29948,  3009,  1116,  1763, 29946,\n",
      "         9531, 24479, 29840, 21314, 26961,  6119, 29946,  4941, 11055, 17953,\n",
      "        29948,   294,  1002, 13079,  1116,  4243, 29840,   374,  7460,   934,\n",
      "         5392,  6861,   136, 20232,   630,  4243,   374, 23532,  1116,  1002,\n",
      "        17175, 29946,  4888, 17626,  1002,  2786,  7046, 12006,   317, 20718,\n",
      "         7503, 29948, 22899,   624,  1479, 13497,  1732, 29950,  1002, 20887,\n",
      "        17732,  1116,   223,  5901,   374,  1002, 11319,  9797,  1116, 21257,\n",
      "         8666,   223,  8690, 29946, 14226,   490,  1002, 16823,  1991,   598,\n",
      "         1045,  1045,   117,   253, 23251, 29833,  6199, 29946,  3636,  5352,\n",
      "        29840,  1226,  1002, 29534, 20478,  4012,  1116,  1002,  5901,   223,\n",
      "         1669,   135,   591,  1116,  5379,   374,  1002, 20887, 17732,  8666,\n",
      "          223, 28079, 12123, 10925,  7193,  1002, 19721, 29948,  1701, 10925,\n",
      "         1432,   253,  6855,   136, 22899,  3732, 22272,  1732, 29946, 14226,\n",
      "          490, 20940, 17894,  4855,  8666, 20232,   630,  1116,  1044,  4442,\n",
      "         6587, 29948,  9315,    48,  2074, 29836,  4888, 11176,  8666,  1002,\n",
      "        28079, 12123,  5243,  6459,   133, 20076, 16742,   136,   374, 28560,\n",
      "         3437,  1290, 24926, 29946, 24008, 15156, 11500, 29835,   253,  7460,\n",
      "        20887, 17732, 12028,  1226, 10198,  7193,  7467, 29840, 29948,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}, {'input_ids': tensor([    3, 16608, 24045,     7, 29950, 20887, 17732,   374,   253, 26544,\n",
      "         1116,    11, 29942, 29948, 29943, 20718,   133,    48, 19967,  2828,\n",
      "           10, 29946, 19998,  5243,  1002,  7012,    55, 27952, 29950, 29947,\n",
      "           11, 29942,    10, 29943, 29953,   111, 29946, 20741,   101,  1116,\n",
      "          918,  9689,  5707, 29951, 29947,    11, 29942,   253, 29943, 29953,\n",
      "          502,  1548, 20232,   630,   253, 29946, 20741,   101,  1116, 29442,\n",
      "         1479, 29951, 29947,    11, 29942,   253, 29855,   489,   253, 29856,\n",
      "        29948, 29948, 29948, 29943, 29953,    11, 29942,   253, 29855, 29943,\n",
      "        29945,    11, 29942,   253, 29856, 29943, 29945, 29948, 29948, 29948,\n",
      "         1548, 20232,   630, 27807,  2754,  1116, 17690, 29947,  1226, 29947,\n",
      "        17690, 28600,  7623,  9533, 19996, 29946, 20741,   101,  1116,  1002,\n",
      "         7063, 14383, 29948, 13497,  2754,  1116, 20887, 17732, 29950,  1002,\n",
      "        20887, 17732,  1116,   223,  5901,   253,   374,  1002, 17260,  1116,\n",
      "         1002, 13079,  1116, 13687, 29840, 18989,  4864,  1226,  1002, 29534,\n",
      "        20478,  4012,  1116,   253,  1226,  1002,  2405, 13079,  1116,   598,\n",
      "          106,  9084, 13687, 29840, 29946,  3636,  3999,  2692,  8666, 29611,\n",
      "         5811, 12535,  1742, 29849,  9670,  1290, 29948,  1669,   135,   591,\n",
      "        29950, 19411,  1903,  1116,  1002, 11097,   630,  1116, 17690,  1100,\n",
      "        29835, 29950, 29947,   253, 29950,   671,  1116, 13079, 29840,   374,\n",
      "          606, 29951, 29947,    13, 29950,   671,  1116, 13079, 29840,   374,\n",
      "        25326,   153,  6706,   606, 29947, 29947, 29954,    11, 29942,   253,\n",
      "        29943, 29953,   418, 29949,  4287, 29953,   502, 29948, 19834, 29946,\n",
      "           13, 29953,   253, 29951,    11, 29942,    13, 29943, 29953,    11,\n",
      "        29942,   253, 29943, 29953,   111, 29947,    11, 29942,   253, 29943,\n",
      "        29953,   111, 29947,   418, 29949,  4287, 29953,   502, 29948, 11457,\n",
      "        29855,  2786,  7046, 12006,   317, 20718,  7503, 29840, 29950,  1002,\n",
      "        20887, 17732, 29942,   253, 29943,  1116,   223,  5901,   253,   374,\n",
      "         1002, 17887,  1116,  1002, 19705,  2786,  7046,  5683, 29849,  4232,\n",
      "        19998,   253, 29534,  4421,    48,   253,  5634, 10356,  1116,  1271,\n",
      "         5249,   133,  4243, 29840,  7906,  1002,  1620, 29835, 27573, 29840,\n",
      "        29948,  1669,   135,   591, 29950, 23676,  5039, 18172, 29840, 29947,\n",
      "         2786,  7046, 12006,  9102, 26120, 29950,  1002, 20887, 18236, 13120,\n",
      "         5811, 19705,  2786,  7046,  5683,  5953,  1116, 27807,    48,  1002,\n",
      "         4243, 29840, 29947,  2829,  2182,  3871,  2786,  7046, 12006,  9102,\n",
      "        29950,  1002, 20887, 18236, 13120,  5811, 19705,  2786,  7046,  5683,\n",
      "         5953,  1116, 27807,   490,   223, 26465,  1264,  3658,  6431,  4132,\n",
      "        14963,    35,  7046,  5683, 29835,  1116,  4243, 29840, 29948,  1002,\n",
      "        17887, 29840,  5811, 29950,  1002,  4243, 29840,  8799,  1432,  2035,\n",
      "        12717, 29842,  3009,  7906,  1002,  1620, 29835, 27573, 29840, 29946,\n",
      "           48,  1002,  4182,  1264,   569,  4325,   278,  9590,  2786,  7046,\n",
      "        12006,  1701,  2481, 14234,  2333,  8666,  1002, 13079,  1116,  4243,\n",
      "        29840,   374, 20076, 21443, 29948,  1002, 20887, 17732,  1116,   223,\n",
      "         5901,   374,  1002, 11319,  9797,  1116, 14983, 29271,  8666,   223,\n",
      "         8690, 29946,   490,  1002,  8218,  1116, 16823,  4115, 29842,   117,\n",
      "          253, 23251, 29833,  6199, 29946,  3636,  5352, 29840,  1226,  1002,\n",
      "        29534, 20478,  4012,  1116,  1002,  5901, 29948,  1548,  1002, 22899,\n",
      "         3732, 20887, 17732,  4888,  4442, 15139,  1116,  1709, 29840, 29948,\n",
      "         1002, 25100, 16686,  5811, 29950,  1002, 27472,   136,  1002, 14714,\n",
      "        29948,    48,  4755,  1548,  1002,  1709,  1226,  1432,  9911, 29946,\n",
      "         1701,   374, 27518,  1154,  3700,  8666,  1701,   374,   598,   106,\n",
      "         9084,  1226,  3913, 15480,  1002, 25788, 29840, 17122,  1290,  4232,\n",
      "          502,     3]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'token_type_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}, {'input_ids': tensor([    3,  1620, 29835, 27573, 29840, 29946,    48,  1002,  4182,  1264,\n",
      "          569,  4325,   278,  9590,  2786,  7046, 12006,  1701,  2481, 14234,\n",
      "         2333,  8666,  1002, 13079,  1116,  4243, 29840,   374, 20076, 21443,\n",
      "        29948,  1002, 20887, 17732,  1116,   223,  5901,   374,  1002, 11319,\n",
      "         9797,  1116, 14983, 29271,  8666,   223,  8690, 29946,   490,  1002,\n",
      "         8218,  1116, 16823,  4115, 29842,   117,   253, 23251, 29833,  6199,\n",
      "        29946,  3636,  5352, 29840,  1226,  1002, 29534, 20478,  4012,  1116,\n",
      "         1002,  5901, 29948,  1548,  1002, 22899,  3732, 20887, 17732,  4888,\n",
      "         4442, 15139,  1116,  1709, 29840, 29948,  1002, 25100, 16686,  5811,\n",
      "        29950,  1002, 27472,   136,  1002, 14714, 29948,    48,  4755,  1548,\n",
      "         1002,  1709,  1226,  1432,  9911, 29946,  1701,   374, 27518,  1154,\n",
      "         3700,  8666,  1701,   374,   598,   106,  9084,  1226,  3913, 15480,\n",
      "         1002, 25788, 29840, 17122,  1290,  4232,   502, 29948,   418, 20887,\n",
      "        17732, 29946,  5379,  8370, 21799,  1002,  8981, 11075, 26617,  1002,\n",
      "        25788,  1116,  1002, 27472, 29946,  7906, 13188,  3065,    53,  1002,\n",
      "        20887, 17732,  1116,  1002,  5901,   569,  8666,  1002,  8660,  5718,\n",
      "        13188,  3065,    53,  1701, 29948,  1002,  1709,  6958,  9824,  2815,\n",
      "         1149, 29853, 29849,  1002, 19705, 14765, 13062,  1548,  1002, 20567,\n",
      "          136, 11152,   376,  1732, 29950,  6298, 29840, 29936,   921,   253,\n",
      "           31, 29936,    48, 18989,  1116,   253,  3021,  1854, 19587,  1226,\n",
      "          253, 11319,  9797,  1116,  1320,  1083,  1548,   253, 29948,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}]), (tensor(80.), tensor(80.), tensor(100.), tensor(100.))]\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HierarchicalBert(\n",
       "  (bert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertSdpaAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (regression_layer): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HierarchicalBert(\"indobenchmark/indobert-lite-base-p2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.MSELoss()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len tensor :  1166\n",
      "len tensor :  1018\n",
      "Predictions: tensor([-0.1692, -0.1689], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.8000], device='cuda:0')\n",
      "loss :  0.9390792846679688\n",
      "len tensor :  1065\n",
      "len tensor :  1070\n",
      "Predictions: tensor([0.1305, 0.1319], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.7548768520355225\n",
      "len tensor :  1020\n",
      "len tensor :  1025\n",
      "Predictions: tensor([0.4216, 0.4258], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 1.0000], device='cuda:0')\n",
      "loss :  0.23644286394119263\n",
      "len tensor :  1056\n",
      "len tensor :  1123\n",
      "Predictions: tensor([0.6767, 0.6784], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.9100], device='cuda:0')\n",
      "loss :  0.03441764786839485\n",
      "len tensor :  1055\n",
      "len tensor :  1045\n",
      "Predictions: tensor([0.8852, 0.8847], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.8000], device='cuda:0')\n",
      "loss :  0.007216768339276314\n",
      "len tensor :  1131\n",
      "len tensor :  1047\n",
      "Predictions: tensor([1.0261, 1.0225], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.9100, 0.9100], device='cuda:0')\n",
      "loss :  0.01307363249361515\n",
      "len tensor :  1057\n",
      "len tensor :  1167\n",
      "Predictions: tensor([1.1004, 1.1027], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.9100], device='cuda:0')\n",
      "loss :  0.02361566387116909\n",
      "len tensor :  1210\n",
      "len tensor :  1014\n",
      "Predictions: tensor([1.1301, 1.0296], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.5900], device='cuda:0')\n",
      "loss :  0.15111839771270752\n",
      "len tensor :  1252\n",
      "len tensor :  1119\n",
      "Predictions: tensor([0.7149, 0.7157], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.9100], device='cuda:0')\n",
      "loss :  0.018986981362104416\n",
      "len tensor :  1115\n",
      "len tensor :  1027\n",
      "Predictions: tensor([0.7073, 0.9968], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.7000], device='cuda:0')\n",
      "loss :  0.04407641291618347\n",
      "len tensor :  1000\n",
      "len tensor :  1096\n",
      "Predictions: tensor([0.9590, 0.6780], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.8000], device='cuda:0')\n",
      "loss :  0.04098251834511757\n",
      "len tensor :  1089\n",
      "len tensor :  1125\n",
      "Predictions: tensor([0.6428, 0.6406], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.8000], device='cuda:0')\n",
      "loss :  0.014348560012876987\n",
      "len tensor :  1829\n",
      "len tensor :  1121\n",
      "Predictions: tensor([0.3922, 0.6649], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.6000], device='cuda:0')\n",
      "loss :  0.18681472539901733\n",
      "len tensor :  1721\n",
      "len tensor :  1766\n",
      "Predictions: tensor([0.5018, 0.5049], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.24665291607379913\n",
      "len tensor :  1762\n",
      "len tensor :  1458\n",
      "Predictions: tensor([0.5300, 0.5307], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.22057586908340454\n",
      "len tensor :  1669\n",
      "len tensor :  1518\n",
      "Predictions: tensor([0.5727, 0.5709], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.7900], device='cuda:0')\n",
      "loss :  0.11530966311693192\n",
      "len tensor :  1582\n",
      "len tensor :  1023\n",
      "Predictions: tensor([0.6169, 1.1242], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.2100], device='cuda:0')\n",
      "loss :  0.49131491780281067\n",
      "len tensor :  1312\n",
      "len tensor :  1245\n",
      "Predictions: tensor([0.8242, 0.8241], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.6000], device='cuda:0')\n",
      "loss :  0.040566205978393555\n",
      "len tensor :  1346\n",
      "len tensor :  1305\n",
      "Predictions: tensor([0.8327, 0.8327], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.7900], device='cuda:0')\n",
      "loss :  0.014916582964360714\n",
      "len tensor :  1324\n",
      "len tensor :  1272\n",
      "Predictions: tensor([0.8436, 0.8433], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.024507611989974976\n",
      "len tensor :  1797\n",
      "len tensor :  1483\n",
      "Predictions: tensor([0.6673, 0.6661], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.1110713928937912\n",
      "len tensor :  1406\n",
      "len tensor :  1722\n",
      "Predictions: tensor([0.9082, 0.7025], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.4000, 1.0000], device='cuda:0')\n",
      "loss :  0.1733851581811905\n",
      "len tensor :  1176\n",
      "len tensor :  1473\n",
      "Predictions: tensor([0.9336, 0.7214], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7900, 1.0000], device='cuda:0')\n",
      "loss :  0.04911821335554123\n",
      "len tensor :  1460\n",
      "len tensor :  1184\n",
      "Predictions: tensor([0.7440, 0.9599], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.7900], device='cuda:0')\n",
      "loss :  0.047213196754455566\n",
      "len tensor :  1514\n",
      "len tensor :  1367\n",
      "Predictions: tensor([0.7638, 0.9852], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.028005508705973625\n",
      "len tensor :  1285\n",
      "len tensor :  1376\n",
      "Predictions: tensor([1.0129, 1.0125], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7900, 1.0000], device='cuda:0')\n",
      "loss :  0.02492215298116207\n",
      "len tensor :  1346\n",
      "len tensor :  1056\n",
      "Predictions: tensor([1.0308, 1.0299], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.4000, 0.2100], device='cuda:0')\n",
      "loss :  0.5350939035415649\n",
      "len tensor :  1345\n",
      "len tensor :  1454\n",
      "Predictions: tensor([1.0061, 0.7819], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.023808453232049942\n",
      "len tensor :  1044\n",
      "len tensor :  1190\n",
      "Predictions: tensor([1.3532, 0.9878], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.4000, 0.4000], device='cuda:0')\n",
      "loss :  0.6270339488983154\n",
      "len tensor :  1656\n",
      "len tensor :  1158\n",
      "Predictions: tensor([0.7204, 0.9278], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.7000], device='cuda:0')\n",
      "loss :  0.06503056734800339\n",
      "len tensor :  1463\n",
      "len tensor :  1359\n",
      "Predictions: tensor([0.6744, 0.8728], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.06108412519097328\n",
      "len tensor :  1215\n",
      "len tensor :  1592\n",
      "Predictions: tensor([0.8354, 0.6459], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7900, 1.0000], device='cuda:0')\n",
      "loss :  0.06370599567890167\n",
      "len tensor :  1322\n",
      "len tensor :  1422\n",
      "Predictions: tensor([0.8130, 0.8132], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.0349312424659729\n",
      "len tensor :  1403\n",
      "len tensor :  1337\n",
      "Predictions: tensor([0.8032, 0.8033], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.7900], device='cuda:0')\n",
      "loss :  0.019457178190350533\n",
      "len tensor :  1143\n",
      "len tensor :  1143\n",
      "Predictions: tensor([0.7987, 0.7983], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.4900, 0.6000], device='cuda:0')\n",
      "loss :  0.06729979068040848\n",
      "len tensor :  1224\n",
      "len tensor :  1421\n",
      "Predictions: tensor([0.7803, 0.7806], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.7900], device='cuda:0')\n",
      "loss :  0.02417096123099327\n",
      "len tensor :  1228\n",
      "len tensor :  1280\n",
      "Predictions: tensor([0.7700, 0.7704], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.6000, 0.7900], device='cuda:0')\n",
      "loss :  0.014643258415162563\n",
      "len tensor :  1718\n",
      "len tensor :  1379\n",
      "Predictions: tensor([0.5853, 0.7398], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.11985041201114655\n",
      "len tensor :  1210\n",
      "len tensor :  1121\n",
      "Predictions: tensor([0.7428, 1.0268], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7900, 1.0000], device='cuda:0')\n",
      "loss :  0.0014757071621716022\n",
      "len tensor :  1218\n",
      "len tensor :  1229\n",
      "Predictions: tensor([0.7443, 0.7442], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.0654180496931076\n",
      "len tensor :  1054\n",
      "len tensor :  1201\n",
      "Predictions: tensor([1.0485, 1.0499], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.2100, 1.0000], device='cuda:0')\n",
      "loss :  0.3527793288230896\n",
      "len tensor :  1275\n",
      "len tensor :  1078\n",
      "Predictions: tensor([0.7462, 1.0120], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.032284658402204514\n",
      "len tensor :  1166\n",
      "len tensor :  1018\n",
      "Predictions: tensor([1.0476, 1.0451], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.8000], device='cuda:0')\n",
      "loss :  0.06069004163146019\n",
      "len tensor :  1065\n",
      "len tensor :  1070\n",
      "Predictions: tensor([1.0201, 1.0206], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1., 1.], device='cuda:0')\n",
      "loss :  0.00041444989619776607\n",
      "len tensor :  1020\n",
      "len tensor :  1025\n",
      "Predictions: tensor([0.9908, 0.9936], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 1.0000], device='cuda:0')\n",
      "loss :  0.018215302377939224\n",
      "len tensor :  1056\n",
      "len tensor :  1123\n",
      "Predictions: tensor([0.9590, 0.9617], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.9100], device='cuda:0')\n",
      "loss :  0.013980643823742867\n",
      "len tensor :  1055\n",
      "len tensor :  1045\n",
      "Predictions: tensor([0.9222, 0.9220], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.8000], device='cuda:0')\n",
      "loss :  0.014914458617568016\n",
      "len tensor :  1131\n",
      "len tensor :  1047\n",
      "Predictions: tensor([0.8815, 0.8776], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.9100, 0.9100], device='cuda:0')\n",
      "loss :  0.000932941387873143\n",
      "len tensor :  1057\n",
      "len tensor :  1167\n",
      "Predictions: tensor([0.8405, 0.8440], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.9100], device='cuda:0')\n",
      "loss :  0.014892352744936943\n",
      "len tensor :  1210\n",
      "len tensor :  1014\n",
      "Predictions: tensor([0.8204, 0.8150], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.8000, 0.5900], device='cuda:0')\n",
      "loss :  0.02552027814090252\n",
      "len tensor :  1252\n",
      "len tensor :  1119\n",
      "Predictions: tensor([0.5625, 0.5619], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.9100], device='cuda:0')\n",
      "loss :  0.07003432512283325\n",
      "len tensor :  1115\n",
      "len tensor :  1027\n",
      "Predictions: tensor([0.5571, 0.7820], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.7000], device='cuda:0')\n",
      "loss :  0.01357016060501337\n",
      "len tensor :  1000\n",
      "len tensor :  1096\n",
      "Predictions: tensor([0.7758, 0.5521], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.8000], device='cuda:0')\n",
      "loss :  0.03361332416534424\n",
      "len tensor :  1089\n",
      "len tensor :  1125\n",
      "Predictions: tensor([0.5556, 0.5542], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([0.7000, 0.8000], device='cuda:0')\n",
      "loss :  0.04063302278518677\n",
      "len tensor :  1829\n",
      "len tensor :  1121\n",
      "Predictions: tensor([0.3540, 0.5832], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Targets: tensor([1.0000, 0.6000], device='cuda:0')\n",
      "loss :  0.20878718793392181\n",
      "len tensor :  1721\n",
      "len tensor :  1766\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(targets)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictions)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTargets:\u001b[39m\u001b[38;5;124m\"\u001b[39m, targets)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[127], line 26\u001b[0m, in \u001b[0;36mHierarchicalBert.forward\u001b[1;34m(self, batch_chunks)\u001b[0m\n\u001b[0;32m     23\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# masuk ke arsitektur bert -> outputnya [max_seq x [768 token]]\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03mambil token CLS nya saja karena pada token ini berisi \"rangkuman\" informasi dari keseluruhan token (token cls ini ada di index 0)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m(alt) 511 token lainnya bisa saja digunakan, namun harus masuk ke mean pooling/attention pooling dulu agar ukuran token berubah dari 512x768 ke 1x768\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m chunk_outputs\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:804\u001b[0m, in \u001b[0;36mAlbertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    800\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m extended_attention_mask) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[0;32m    802\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 804\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    813\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    815\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler_activation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output[:, \u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:535\u001b[0m, in \u001b[0;36mAlbertTransformer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Index of the hidden group\u001b[39;00m\n\u001b[0;32m    533\u001b[0m group_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(i \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_groups))\n\u001b[1;32m--> 535\u001b[0m layer_group_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malbert_layer_groups\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayers_per_group\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlayers_per_group\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_group_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:487\u001b[0m, in \u001b[0;36mAlbertLayerGroup.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m    484\u001b[0m layer_attentions \u001b[38;5;241m=\u001b[39m ()\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_index, albert_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malbert_layers):\n\u001b[1;32m--> 487\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[43malbert_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:450\u001b[0m, in \u001b[0;36mAlbertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    444\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     output_hidden_states: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 450\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m     ffn_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_chunk,\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward,\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim,\n\u001b[0;32m    456\u001b[0m         attention_output[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    457\u001b[0m     )\n\u001b[0;32m    458\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_layer_layer_norm(ffn_output \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\transformers\\models\\albert\\modeling_albert.py:393\u001b[0m, in \u001b[0;36mAlbertSdpaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    391\u001b[0m batch_size, seq_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    392\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(hidden_states))\n\u001b[1;32m--> 393\u001b[0m key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    394\u001b[0m value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;66;03m# Reference: https://github.com/pytorch/pytorch/issues/112577\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\Code\\env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    for batch, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # batch = batch.to(device)\n",
    "        targets = torch.stack(targets).to(device)\n",
    "        # forward pass\n",
    "        predictions = model(batch).squeeze(1)\n",
    "\n",
    "        print(\"Predictions:\", predictions)\n",
    "        print(\"Targets:\", targets)\n",
    "        # compute loss\n",
    "        loss = criterion(predictions, targets)\n",
    "        print(\"loss : \", loss.item())\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
