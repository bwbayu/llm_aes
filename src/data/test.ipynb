{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# import sys\n",
    "# sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel, AutoModel, AutoTokenizer\n",
    "import torch\n",
    "# \n",
    "from dataset import EssayDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>answer</th>\n",
       "      <th>score</th>\n",
       "      <th>dataset</th>\n",
       "      <th>max_length1</th>\n",
       "      <th>normalized_score</th>\n",
       "      <th>normalized_score2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>sumber tenaga, pemanis alami, menjaga sistem i...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>65</td>\n",
       "      <td>0.27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>sebagai sumber energi, pemanis alami, menjaga ...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>66</td>\n",
       "      <td>0.21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>1. Sebagai energi. 2. Sebagai memperlancaar pe...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>76</td>\n",
       "      <td>0.42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>untuk membuat kenyang, agar tidak lapar, agar ...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>67</td>\n",
       "      <td>0.18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jelaskan kegunaan karbohidrat untuk tubuh kita.</td>\n",
       "      <td>Fungsi karbohidrat adalah sebagai pemasok ener...</td>\n",
       "      <td>Karbohidrat mempunyai peran penting untuk pros...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>analisis_essay</td>\n",
       "      <td>105</td>\n",
       "      <td>0.82</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question  \\\n",
       "0  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "1  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "2  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "3  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "4  Jelaskan kegunaan karbohidrat untuk tubuh kita.   \n",
       "\n",
       "                                    reference_answer  \\\n",
       "0  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "1  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "2  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "3  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "4  Fungsi karbohidrat adalah sebagai pemasok ener...   \n",
       "\n",
       "                                              answer  score         dataset  \\\n",
       "0  sumber tenaga, pemanis alami, menjaga sistem i...   27.0  analisis_essay   \n",
       "1  sebagai sumber energi, pemanis alami, menjaga ...   21.0  analisis_essay   \n",
       "2  1. Sebagai energi. 2. Sebagai memperlancaar pe...   42.0  analisis_essay   \n",
       "3  untuk membuat kenyang, agar tidak lapar, agar ...   18.0  analisis_essay   \n",
       "4  Karbohidrat mempunyai peran penting untuk pros...   82.0  analisis_essay   \n",
       "\n",
       "   max_length1  normalized_score  normalized_score2  \n",
       "0           65              0.27                 27  \n",
       "1           66              0.21                 21  \n",
       "2           76              0.42                 42  \n",
       "3           67              0.18                 18  \n",
       "4          105              0.82                 82  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/aes_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masuk\n",
      "masuk 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2, 29791, 29950,  7370,  8600,  8871,    90,   825,   219, 29948,\n",
      "        10948,  4012, 24045,     7, 29950,  1539,  8871,   154,   242, 10377,\n",
      "         2479, 29946,   173, 15743,   699,   126,  6914, 29946,   651,  1280,\n",
      "        14939,    79,  2674,   157, 26237, 29947,  1107,    41, 29614,  6485,\n",
      "         2788,    41, 10845,   112,   825,     3, 16608, 24045,     7, 29950,\n",
      "         1099,  1809, 29946, 21137,  1798, 29946,  2079,   737,  7725, 29946,\n",
      "           41,   242,  6795,   825,     3,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'scores': tensor(27.), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"masuk\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-lite-base-p2\")\n",
    "except TypeError:\n",
    "    print(\"masuk 2\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-lite-base-p2\")\n",
    "dataset = EssayDataset(df, tokenizer, 512)\n",
    "print(dataset[0]) # 135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(profile=\"full\")\n",
    "print(dataset[0]['token_type_ids']) # \n",
    "torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def custom_truncate_and_tokenize(tokenizer, question, reference_answer, student_answer, max_length=100):\n",
    "    \"\"\"\n",
    "    Tokenizes and truncates the inputs while ensuring:\n",
    "    - The question is not truncated.\n",
    "    - The reference and student answers share the remaining space equally.\n",
    "    - If one is shorter than its allocated space, the other gets the remaining space.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (AutoTokenizer): Pretrained tokenizer.\n",
    "        question (str): Question text.\n",
    "        reference_answer (str): Reference answer text.\n",
    "        student_answer (str): Student answer text.\n",
    "        max_length (int): Maximum sequence length (default: 512 for BERT).\n",
    "\n",
    "    Returns:\n",
    "        dict: Tokenized inputs with proper truncation.\n",
    "    \"\"\"\n",
    "    # Tokenize separately without truncation\n",
    "    question_tokens = tokenizer.tokenize(question)\n",
    "    ref_tokens = tokenizer.tokenize(reference_answer)\n",
    "    stud_tokens = tokenizer.tokenize(student_answer)\n",
    "\n",
    "    # Special tokens [CLS] and [SEP]\n",
    "    cls_token = [tokenizer.cls_token]  # [CLS]\n",
    "    sep_token = [tokenizer.sep_token]  # [SEP]\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Ensure the question is not truncated\n",
    "    q_length = len(question_tokens)\n",
    "    remaining_length = max_length - (q_length + 3)  # 3 = [CLS] + [SEP] + [SEP]\n",
    "\n",
    "    # Initial equal split\n",
    "    half_remaining = remaining_length // 2\n",
    "    ref_alloc = min(len(ref_tokens), half_remaining)\n",
    "    stud_alloc = min(len(stud_tokens), half_remaining)\n",
    "\n",
    "    # If extra space is left, give it to the longer one\n",
    "    extra_space = remaining_length - (ref_alloc + stud_alloc)\n",
    "    if len(ref_tokens) > ref_alloc:\n",
    "        ref_alloc += extra_space\n",
    "    elif len(stud_tokens) > stud_alloc:\n",
    "        stud_alloc += extra_space\n",
    "\n",
    "    # Truncate\n",
    "    truncated_ref = ref_tokens[:ref_alloc]\n",
    "    truncated_stud = stud_tokens[:stud_alloc]\n",
    "\n",
    "    # Reconstruct final token sequence\n",
    "    final_tokens = cls_token + question_tokens + truncated_ref + sep_token + truncated_stud + sep_token\n",
    "\n",
    "    # Convert to input IDs\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(final_tokens)\n",
    "\n",
    "    # Attention mask (1 for tokens, 0 for padding)\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Apply padding if the length is less than max_length\n",
    "    pad_length = max_length - len(input_ids)\n",
    "    if pad_length > 0:\n",
    "        input_ids += [pad_token_id] * pad_length\n",
    "        attention_mask += [0] * pad_length\n",
    "\n",
    "    sep_token = tokenizer.encode_plus(tokenizer.sep_token, add_special_tokens=False)['input_ids'][0]\n",
    "    pad_token = tokenizer.encode_plus(tokenizer.pad_token, add_special_tokens=False)['input_ids'][0]\n",
    "\n",
    "    # create token_type_ids manually, this token is used to differentiate between segment\n",
    "    token_type_ids = []\n",
    "    current_token = 0\n",
    "    flag = 0\n",
    "    for token in input_ids:\n",
    "        if(token == pad_token):\n",
    "            token_type_ids.append(pad_token_id)\n",
    "            continue\n",
    "        token_type_ids.append(current_token)\n",
    "        if((token == sep_token) and (flag == 0)):\n",
    "            current_token += 1\n",
    "            flag = 1\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "# Load a pretrained BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example texts\n",
    "question_text = \"What is the capital of France?\"\n",
    "reference_answer_text = \"The capital of France is Paris. Paris is known for its culture and history.\"\n",
    "student_answer_text = \"Paris is the capital. It has many tourist attractions.\"\n",
    "\n",
    "# Apply custom truncation and tokenization\n",
    "output = custom_truncate_and_tokenize(tokenizer, question_text, reference_answer_text, student_answer_text)\n",
    "\n",
    "print(output)\n",
    "print(\"Tokenized Length:\", len(output[\"input_ids\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
